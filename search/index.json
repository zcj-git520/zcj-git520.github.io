[{"content":"虚拟地址空间布局  程序通过编译成为一堆的机器指令写入可执行文件，程序在运行是会将可执行文件加载在计算机的内存中 在虚拟地址空间分布中处于代码段。 程序中的局部变量、函数的参数、函数的返回值等数据会保存在虚拟地址的栈中(栈是先进后出的数据结构) 栈空间的编译器分配和释放。 程序的全局变量和静态变量会保存在虚拟地址的数据段 动态分配内存的地址会保存在虚拟地址空间的堆上。堆空间是动态开辟的内存空间，需要主动开辟和释放。或者 调用GC释放 \r  堆空间内存管理  堆内存空间不是编译器分配，而是有程序动态分配的内存空间。  手动垃圾回收  需要程序主动释放没有用的数据所在的堆空间。如：c++中调用new()函数向计算机申请开辟内存空间后，使用delete或delete[]释放不需要的 堆内存空间。这一类是手动内存分配和释放。手动内存分配使用不恰当也会造成：内存泄露 悬挂指针的问题   过早释放会造成悬挂指针（野指针）：提前释放了动态的堆内存的空间，当程序访问这段地址时会报错。因为这段提前释放的内存空间被清空、 重新分配或者被操作系统回收。释放指针时将指针赋值为NULL，在访问时对指针进行判断是否为NULL 不释放内存会造成内存泄漏：堆内存需要手动释放，当程序运行结束不释放，这段内存就会被一直占用。如果 一直在分配不释放会一直占用计算机的内存，直到内存被占完。将new与delete配套使用，使用工具检测或者打印出堆信息  自动垃圾回收（GC）  在程序运行过程中自动释放没有用的数据所在的堆空间（垃圾回收).在虚拟内存空间中能从栈或者数据段的根节点追踪不到的数据为没用的数据 （内存垃圾），常用的算法：标记法, 计数法  标记法回收  标记法：将栈或者数据段作为根（root）进行追踪,将能追踪得的数据（堆空间）进行标记。没有被标记的数据 （堆空间）就是垃圾，将这部分垃圾进行回收。三色抽象：   垃圾回收开始时，将所有数据为白色 垃圾回收开始时，将所有的栈或者数据段的根节点设置为灰色 在根据根节点进行追踪，直到所有的数据节点追踪结束后将根节点置为黑色，在将根节点的下一节点作为根节点进行追踪 所有的数据节点都追踪完后，会剩下黑色和白色的数据节点。黑色表示有用的数据。白色为无用的数据。将白色的数据进行回收（堆空间的释放） \r   标记法实现简单，但是会造成内存的碎片化(内存块中是可使用小内存块，造成大内存块不能使用这块内存，这些小小内存块也不能使用) 因为内存碎片化的问题诞生了   标记整理法，就是标记法之后，将有用的数据堆内存空间移动在一起，释放更多连续的堆空间,但是这种做法带来 很大的开销，因为需要不断的扫描内存和移动内存 复制回收法。将堆内存分为from和To两个相同的堆内存空间。程序执行时，使用from的堆空间。垃圾回收时会扫描from 的堆内存空间，将有用的数据复制到To的堆空间上。垃圾回收结束时，将To堆空间设置为From堆空间。将原来的from 堆空间全部回收后置为Ton堆空间。但是复制回收法只会使用一般的堆内存空间，造成堆内存空间利用率不高 \r 分代法回收：大部分对象都会在年轻时候死亡（弱分代假说）把新建的对象称之为新生代对象。经过特定次数的GC(垃圾回收)数据依然有用的对象称为 老年代对象。而大部分会在新生代对象就会垃圾回收了，在结合复制回收法使用 \r  计数法回收  引用计数指的是对象被引用的次数，程序在运行过程中会更新引用次数。对象引用越多，计数越大，当计数为0时，回收该对象（堆内存空间） \r  ","date":"2021-10-18T22:00:38+08:00","image":"https://zcj-git520.github.io/p/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/1_hu86f620222550b5f6f6b08ef9da75c8f9_105109_120x120_fill_box_smart1_3.png","permalink":"https://zcj-git520.github.io/p/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/","title":"内存管理"},{"content":"go Channel  （Do not communicate by sharing memory; instead, share memory by communicating）  CSP并发模型  CSP 即通信顺序进程、交谈循序程序，又被译为交换消息的循序程序(communicating sequential processes)，它是一种用来描述并发性系统之间进行交互的模型。 go Channe是一种特殊的类型，是有特定类型的队列。是链接goroutine(协程)的通信机制，通过通信共享内存而不是通过共享内存而实现通信. Channel 收发操作均遵循了先进先出的设计，具体规则如下：  先从 Channel 读取数据的 Goroutine 会先接收到数据； 先向 Channel 发送数据的 Goroutine 会得到先发送数据的权利；    \r\nchannel 数据结构定义： type hchan struct { // 队列中存储的数量 qcount uint // total data in the queue //环形队列的大小(最大存储数量 ) dataqsiz uint // size of the circular queue // 存放环形队列的数据，数组 buf unsafe.Pointer // points to an array of dataqsiz elements // 元素的大小 elemsize uint16 // 是否关闭的标识 closed uint32 // 元素的类型(指向类型的元数据 ) elemtype *_type // element type // 当前发送数据在环形队列的索引 sendx uint // send index // 当前接受数据在环形队列的索引 recvx uint // receive index // 接收者等待队列（\u0026lt;-ch）阻塞在channel的协程队列 recvq waitq // list of recv waiters // 发送者等待队列（ch\u0026lt;- data）阻塞在channel的协程队列 sendq waitq // list of send waiters //锁保护hchan中的所有字段，以及几个 //在这个通道上阻塞sudogs中的字段 //保持这个锁时不要改变另一个G的状态 //(特别是，不要准备一个G)，因为这可能会死锁 //栈收缩。 lock mutex // 保护hchan中的所有字段，保持协程的状态不被更改，避免造成栈收缩引起的死锁，使用互斥锁解决程序中可能存在的线程竞争问题是很常见的 } 发送者/接收者等待队列的结构：一个双向链表 type waitq struct { first *sudog last *sudog } channel sudog(等待队列)结构如下 type sudog struct { // The following fields are protected by the hchan.lock of the // channel this sudog is blocking on. shrinkstack depends on // this for sudogs involved in channel ops. //以下字段受hchan保护。锁的 //这个sudog正在阻塞。shrinkstack取决于 //这是为涉及通道操作的sudogs。 g *g // 等待的协程协程 next *sudog prev *sudog // 数据元素(可以指向堆栈)，等待发送/接收的数据 elem unsafe.Pointer // data element (may point to stack) // The following fields are never accessed concurrently. // For channels, waitlink is only accessed by g. // For semaphores, all fields (including the ones above) // are only accessed when holding a semaRoot lock. //下面的字段永远不会并发访问。 //对于通道，waitlink只被g访问。 //对于信号量，所有的字段(包括上面的字段) //只在持有semaRoot锁时访问。 acquiretime int64 releasetime int64 ticket uint32 // isSelect indicates g is participating in a select, so // g.selectDone must be CAS'd to win the wake-up race. // 表示g被选择 isSelect bool // success indicates whether communication over channel c // succeeded. It is true if the goroutine was awoken because a // value was delivered over channel c, and false if awoken // because c was closed. //成功表示是否通过通道c通信 // 成功了。 如果 goroutine 被唤醒是因为一个 // 值通过通道 c 传递，如果被唤醒则返回 false // 因为 c 被关闭了 success bool // c 因关闭而唤醒 parent *sudog // semaRoot binary tree waitlink *sudog // g.waiting list or semaRoot waittail *sudog // semaRoot // 等待的channel被唤醒 c *hchan // channel }  结构如图所示 \r  channel 创建  channel 和 切片、map一样，需要使用make(chan type, int )才能使用,应为make()会调用makeChan()初始化  makech函数源码如下: // 参数类型：创建chan的类型和环型缓冲区的数量 func makechan(t *chantype, size int) *hchan { elem := t.elem // compiler checks this but be safe. if elem.size \u0026gt;= 1\u0026lt;\u0026lt;16 { throw(\u0026quot;makechan: invalid channel element type\u0026quot;) } if hchanSize%maxAlign != 0 || elem.align \u0026gt; maxAlign { throw(\u0026quot;makechan: bad alignment\u0026quot;) } //判断环型缓冲区是否溢出 mem, overflow := math.MulUintptr(elem.size, uintptr(size)) if overflow || mem \u0026gt; maxAlloc-hchanSize || size \u0026lt; 0 { panic(plainError(\u0026quot;makechan: size out of range\u0026quot;)) } // Hchan does not contain pointers interesting for GC when elements stored in buf do not contain pointers. // buf points into the same allocation, elemtype is persistent. // SudoG's are referenced from their owning thread so they can't be collected. // TODO(dvyukov,rlh): Rethink when collector can move allocated objects. var c *hchan switch { case mem == 0: // Queue or element size is zero. // 当队列或者元素大小为0时，定义无缓冲chan（同步chan） c = (*hchan)(mallocgc(hchanSize, nil, true)) // Race detector uses this location for synchronization. // Race 竞争检查利用这个地址来进行同步操作 c.buf = c.raceaddr() case elem.ptrdata == 0: // Elements do not contain pointers. // Allocate hchan and buf in one call. // 元素不包含指针时。一次分配 hchan 和 buf 的内存。 c = (*hchan)(mallocgc(hchanSize+mem, nil, true)) c.buf = add(unsafe.Pointer(c), hchanSize) default: // Elements contain pointers. // 定义带缓存的chan或者异步的chan c = new(hchan) c.buf = mallocgc(mem, elem, true) } c.elemsize = uint16(elem.size) // chan元素的大小 c.elemtype = elem // chan元素的类型 c.dataqsiz = uint(size) // chan缓存区大小 lockInit(\u0026amp;c.lock, lockRankHchan) //初始化互斥锁 if debugChan { print(\u0026quot;makechan: chan=\u0026quot;, c, \u0026quot;; elemsize=\u0026quot;, elem.size, \u0026quot;; dataqsiz=\u0026quot;, size, \u0026quot;\\n\u0026quot;) } return c }  channel创建过程：   编译检查、缓冲区大小检查，判断是否溢出 判断chan的类型\n1、当创建无缓冲chan时,调用mallocgc()在堆上为chan开辟hchanSize的buf缓存内存空间\n2、创建带缓冲的chan时,判断元素的类型是否为指针类型，若不是，则mallocgc()在堆上为chan和buf缓冲区数组开辟一段大小为 hchanSize+mem连续的内存空间。若是则调用mallocgc()在堆上分别为chan和buf缓冲区分配连续内存空间。 \r  channel 发送数据与接收数据 channel 发送数据  chan \u0026lt;- data  chan发送数据源码如下: func chansend(c *hchan, ep unsafe.Pointer, block bool, callerpc uintptr) bool { // 判断chan是否被初始化，向chan为nil的chan发送数据将会永久阻塞 if c == nil { if !block { return false } // 使当前的groutine休眠 gopark(nil, nil, waitReasonChanSendNilChan, traceEvGoStop, 2) throw(\u0026quot;unreachable\u0026quot;) } if debugChan { print(\u0026quot;chansend: chan=\u0026quot;, c, \u0026quot;\\n\u0026quot;) } // 检查在没有获取锁的情况下会导致发送失败的非阻塞操作 if raceenabled { racereadpc(c.raceaddr(), callerpc, funcPC(chansend)) } // Fast path: check for failed non-blocking operation without acquiring the lock. // // After observing that the channel is not closed, we observe that the channel is // not ready for sending. Each of these observations is a single word-sized read // (first c.closed and second full()). // Because a closed channel cannot transition from 'ready for sending' to // 'not ready for sending', even if the channel is closed between the two observations, // they imply a moment between the two when the channel was both not yet closed // and not ready for sending. We behave as if we observed the channel at that moment, // and report that the send cannot proceed. // // It is okay if the reads are reordered here: if we observe that the channel is not // ready for sending and then observe that it is not closed, that implies that the // channel wasn't closed during the first observation. However, nothing here // guarantees forward progress. We rely on the side effects of lock release in // chanrecv() and closechan() to update this thread's view of c.closed and full(). if !block \u0026amp;\u0026amp; c.closed == 0 \u0026amp;\u0026amp; full(c) { return false } var t0 int64 if blockprofilerate \u0026gt; 0 { t0 = cputicks() } // 获得同步锁 lock(\u0026amp;c.lock) // 当chan关闭时,释放锁，并panic // 向也关闭的chan发送消息,会引发panic if c.closed != 0 { unlock(\u0026amp;c.lock) panic(plainError(\u0026quot;send on closed channel\u0026quot;)) } // 如果接收队列中有等待的接收者，直接发送给接收者（有缓存区时，会绕过缓存区） if sg := c.recvq.dequeue(); sg != nil { // Found a waiting receiver. We pass the value we want to send // directly to the receiver, bypassing the channel buffer (if any). send(c, sg, ep, func() { unlock(\u0026amp;c.lock) }, 3) return true } if c.qcount \u0026lt; c.dataqsiz { // 没有接收者，当有缓存区时，将要发送的元素放入队列中 // Space is available in the channel buffer. Enqueue the element to send. qp := chanbuf(c, c.sendx) // 获取缓存地址 if raceenabled { racenotify(c, c.sendx, nil) } typedmemmove(c.elemtype, qp, ep) c.sendx++ // 指向下一个存储的位置 if c.sendx == c.dataqsiz { c.sendx = 0 } c.qcount++ // 缓存数量相加 unlock(\u0026amp;c.lock) return true } if !block { unlock(\u0026amp;c.lock) return false } // 缓存区满了，将当前发送协程加入到等待send队列 // Block on the channel. Some receiver will complete our operation for us. gp := getg() // 获取当前的g发送协程 mysg := acquireSudog()// 创建sudog等待队列 mysg.releasetime = 0 if t0 != 0 { mysg.releasetime = -1 } // No stack splits between assigning elem and enqueuing mysg // on gp.waiting where copystack can find it. mysg.elem = ep mysg.waitlink = nil // 把当前的发送协程与等待队列绑定 mysg.g = gp mysg.isSelect = false mysg.c = c gp.waiting = mysg gp.param = nil // 加入到发送等待队列中 c.sendq.enqueue(mysg) // Signal to anyone trying to shrink our stack that we're about // to park on a channel. The window between when this G's status // changes and when we set gp.activeStackChans is not safe for // stack shrinking. atomic.Store8(\u0026amp;gp.parkingOnChan, 1) gopark(chanparkcommit, unsafe.Pointer(\u0026amp;c.lock), waitReasonChanSend, traceEvGoBlockSend, 2) // Ensure the value being sent is kept alive until the // receiver copies it out. The sudog has a pointer to the // stack object, but sudogs aren't considered as roots of the // stack tracer. KeepAlive(ep) // someone woke us up. // 发送协程被唤醒，解除等待队列的阻塞状态 // 判断的等待队列是否在休眠 if mysg != gp.waiting { throw(\u0026quot;G waiting list is corrupted\u0026quot;) } gp.waiting = nil gp.activeStackChans = false closed := !mysg.success gp.param = nil if mysg.releasetime \u0026gt; 0 { blockevent(mysg.releasetime-t0, 2) } mysg.c = nil releaseSudog(mysg) // 释放等待队列 if closed { if c.closed == 0 { throw(\u0026quot;chansend: spurious wakeup\u0026quot;) } panic(plainError(\u0026quot;send on closed channel\u0026quot;)) } return true }  channel 发送数据总结   判断chan是否被初始化，向chan为nil的chan发送数据将会永久阻塞 检查在没有获取锁， 在没有获取锁的情况下会导致发送失败的非阻塞操作 检查chan是否关闭，向也关闭的chan发送消息,会引发panic 如果接收队列中有等待的接收者，直接发送给接收者（有缓存区时，会绕过缓存区） 没有接收者，当有缓存区时，将要发送的元素放入队列中 缓存区满了，将当前协程加入到send等待队列，并阻塞 当发送协程被唤醒，解除等待队列的阻塞状态，释放等待队列 \r  channel 接收数据  \u0026lt;- data  chan 接收数据源码如下: func chanrecv(c *hchan, ep unsafe.Pointer, block bool) (selected, received bool) { // raceenabled: don't need to check ep, as it is always on the stack // or is new memory allocated by reflect. if debugChan { print(\u0026quot;chanrecv: chan=\u0026quot;, c, \u0026quot;\\n\u0026quot;) } // 判断chan是否初始化，若没有初始化，接收channel数据将阻塞 if c == nil { if !block { return } gopark(nil, nil, waitReasonChanReceiveNilChan, traceEvGoStop, 2) throw(\u0026quot;unreachable\u0026quot;) } // 检查chan是否为空，是否关闭 // Fast path: check for failed non-blocking operation without acquiring the lock. if !block \u0026amp;\u0026amp; empty(c) { // After observing that the channel is not ready for receiving, we observe whether the // channel is closed. // // Reordering of these checks could lead to incorrect behavior when racing with a close. // For example, if the channel was open and not empty, was closed, and then drained, // reordered reads could incorrectly indicate \u0026quot;open and empty\u0026quot;. To prevent reordering, // we use atomic loads for both checks, and rely on emptying and closing to happen in // separate critical sections under the same lock. This assumption fails when closing // an unbuffered channel with a blocked send, but that is an error condition anyway. if atomic.Load(\u0026amp;c.closed) == 0 { // chan关闭，就返回 // Because a channel cannot be reopened, the later observation of the channel // being not closed implies that it was also not closed at the moment of the // first observation. We behave as if we observed the channel at that moment // and report that the receive cannot proceed. return } // The channel is irreversibly closed. Re-check whether the channel has any pending data // to receive, which could have arrived between the empty and closed checks above. // Sequential consistency is also required here, when racing with such a send. if empty(c) { // 如果chan为空 // The channel is irreversibly closed and empty. // // channel 不可逆的关闭了且为空 if raceenabled { raceacquire(c.raceaddr()) } if ep != nil { typedmemclr(c.elemtype, ep) } return true, false } } var t0 int64 if blockprofilerate \u0026gt; 0 { t0 = cputicks() } lock(\u0026amp;c.lock) // chan 关闭了，清理缓冲区 if c.closed != 0 \u0026amp;\u0026amp; c.qcount == 0 { if raceenabled { raceacquire(c.raceaddr()) } unlock(\u0026amp;c.lock) if ep != nil { typedmemclr(c.elemtype, ep) } return true, false } // 找到一个等待的发件人。如果缓冲区大小为 0，则直接从发送方接收值。否则，从队列的头部接收 // 并将发送者的值添加到队列的尾部（两者都映射到 // 相同的缓冲区槽，因为队列已满） // 如果是无缓冲队列，直接从发送方取值 // 如果是待缓冲的区，就从缓冲区头部获取值，并将发送着的值保存在缓冲区后 if sg := c.sendq.dequeue(); sg != nil { // Found a waiting sender. If buffer is size 0, receive value // directly from sender. Otherwise, receive from head of queue // and add sender's value to the tail of the queue (both map to // the same buffer slot because the queue is full). recv(c, sg, ep, func() { unlock(\u0026amp;c.lock) }, 3) return true, true } // 没有发送的协程，但是缓冲区有元素，直接获取缓冲区头部的值 if c.qcount \u0026gt; 0 { // Receive directly from queue qp := chanbuf(c, c.recvx) if raceenabled { racenotify(c, c.recvx, nil) } if ep != nil { typedmemmove(c.elemtype, ep, qp) } typedmemclr(c.elemtype, qp) c.recvx++ if c.recvx == c.dataqsiz { c.recvx = 0 } c.qcount-- unlock(\u0026amp;c.lock) return true, true } if !block { unlock(\u0026amp;c.lock) return false, false } // 当没有发送数据的的协程，且缓冲区值，就将接收的协程放入等待队列中 // no sender available: block on this channel. gp := getg() // 获取当前接收协程 mysg := acquireSudog() // 创建等待队列 mysg.releasetime = 0 if t0 != 0 { mysg.releasetime = -1 } // No stack splits between assigning elem and enqueuing mysg // on gp.waiting where copystack can find it. mysg.elem = ep mysg.waitlink = nil gp.waiting = mysg // 将接送写协程与等待队列绑定 mysg.g = gp mysg.isSelect = false mysg.c = c gp.param = nil // 放入在协程的等待队列中 c.recvq.enqueue(mysg) // Signal to anyone trying to shrink our stack that we're about // to park on a channel. The window between when this G's status // changes and when we set gp.activeStackChans is not safe for // stack shrinking. atomic.Store8(\u0026amp;gp.parkingOnChan, 1) gopark(chanparkcommit, unsafe.Pointer(\u0026amp;c.lock), waitReasonChanReceive, traceEvGoBlockRecv, 2) //当接收协程被唤醒时，解除阻塞状态 // someone woke us up if mysg != gp.waiting { throw(\u0026quot;G waiting list is corrupted\u0026quot;) } gp.waiting = nil gp.activeStackChans = false if mysg.releasetime \u0026gt; 0 { blockevent(mysg.releasetime-t0, 2) } success := mysg.success gp.param = nil mysg.c = nil releaseSudog(mysg) // 释放等待队列内存 return true, success }  channel 发送数据总结   判断chan是否初始化，若没有初始化，接收channel数据将阻塞 检查chan是否为空，是否关闭 当有发送协程，如果是无缓冲队列，直接从发送方取值,如果是待缓冲的区，就从缓冲区头部获取值，并将发送着的值保存在缓冲区后 当没有发送协程，但是有缓冲区有元素，直接获取缓冲区头部的值 当没有发送数据的的协程，且缓冲区值，就将接收的协程放入等待队列中 当接收协程被唤醒时，解除阻塞状态，释放等待队列内存 \r  channel 关闭  close(chan)  chan 关闭源码如下: func closechan(c *hchan) { // 判断chan是否初始化，没有初始化，关闭没有初始化的chan,直接panic if c == nil { panic(plainError(\u0026quot;close of nil channel\u0026quot;)) } // 判断chan是否也被关闭，关闭也关闭的chan,也会发送panic lock(\u0026amp;c.lock) if c.closed != 0 { unlock(\u0026amp;c.lock) panic(plainError(\u0026quot;close of closed channel\u0026quot;)) } if raceenabled { callerpc := getcallerpc() racewritepc(c.raceaddr(), callerpc, funcPC(closechan)) racerelease(c.raceaddr()) } c.closed = 1 var glist gList // 释放所有的接收chan，并将所有的接收队列加入到待清除队列 glist 中 // release all readers for { sg := c.recvq.dequeue() if sg == nil { break } if sg.elem != nil { typedmemclr(c.elemtype, sg.elem) sg.elem = nil } if sg.releasetime != 0 { sg.releasetime = cputicks() } gp := sg.g gp.param = unsafe.Pointer(sg) sg.success = false if raceenabled { raceacquireg(gp, c.raceaddr()) } glist.push(gp) } // 释放所有的发送chan,发送者的等待队列 sendq 中的 sudog 放入待清除队列 glist 中 // release all writers (they will panic) for { sg := c.sendq.dequeue() if sg == nil { break } sg.elem = nil if sg.releasetime != 0 { sg.releasetime = cputicks() } gp := sg.g gp.param = unsafe.Pointer(sg) sg.success = false if raceenabled { raceacquireg(gp, c.raceaddr()) } glist.push(gp) } unlock(\u0026amp;c.lock) 最后会为所有被阻塞的 goroutine 调用 goready 触发调度。将所有 glist 中的 goroutine 状态从 _Gwaiting 设置为 _Grunnable 状态，等待调度器的调度。 // Ready all Gs now that we've dropped the channel lock. for !glist.empty() { gp := glist.pop() gp.schedlink = 0 goready(gp, 3) } }  channel 关闭总结   判断chan是否初始化，没有初始化，关闭没有初始化的chan,直接panic 判断chan是否也被关闭，关闭也关闭的chan,也会发送panic 先释放所有的接收chan，并将所有的接收队列加入到待清除队列 glist 中 释放所有的发送chan,发送者的等待队列 sendq 中的 sudog 放入待清除队列 glist 中 最后会为所有被阻塞的 goroutine 调用 goready 触发调度。将所有 glist 中的goroutine 状态从 _Gwaiting 设置为 _Grunnable 状态，等待调度器的调度。  ","date":"2021-10-15T22:00:38+08:00","image":"https://zcj-git520.github.io/p/go-channel%E7%9A%84%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3/3_huc32f323c0433538c753e80bb5a9a01bb_276268_120x120_fill_box_smart1_3.png","permalink":"https://zcj-git520.github.io/p/go-channel%E7%9A%84%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3/","title":"Go Channel的深入理解"},{"content":"问题  在收集服务的访问记录时，需要将访问记录保存，定义结构体如下    type accessData struct { RemoteAddr string // 远程访问主机地址 RequestURI string //访问的路由 ServerName string // 访问的服务名称 AccessDate string //访问的时间 RunStatus bool //服务是否正常运行 RunError error //运行报错：报错信息. ServerParam interface{} // 访问服务的参数 }   通过结构体转json，同时通过get请求得到图下结果 \r\n  \u0026ldquo;RunError\u0026rdquo;: {},被json转为{}的字符， 打印结构体，发现错误信息是有的：{192.168.1.101:53364 /v1/alarms/out/d GetOutAlarms 2021-10-12 10:09:42 false 没有这个报警🆔id },说明是error 转json问题\n问题分析与解决  问题分析查看error类型定义发现：error类型只是一个接口。它可以包含任何实现它的具体类型的值 解决：将结构体中错误转化为字符串类型，同时用err.Error()返回是错误的字符串  type accessData struct { RemoteAddr string // 远程访问主机地址 RequestURI string //访问的路由 ServerName string // 访问的服务名称 AccessDate string //访问的时间 RunStatus bool //服务是否正常运行 RunError string //运行报错：报错信息. ServerParam interface{} // 访问服务的参数 } type error interface { Error() string }   结果如图\n  \r\n  ","date":"2021-10-09T22:00:38+08:00","image":"https://zcj-git520.github.io/p/golang/2_hu3607656af67f333528ae9e7b0ed06a62_30106_120x120_fill_box_smart1_3.png","permalink":"https://zcj-git520.github.io/p/golang/","title":"go error类型转json"},{"content":"go 协程goroutine  协程是用户级的线程，有用户自己调度，使用协程使得程序调度更加灵活。同时比线程更轻量，占用的栈内存更少。go语言天生支持高并发，go使用协程goroutine的调度器。goroutine 的栈内存最小值为2kb(_StackMin = 2048),它不是固定不变的，可以随需求增大和缩小。goroutine 维护着很大的内存，无需频繁开辟内存，goroutine是使用M:n模型，在用户态切换协程，加上创建协程代价低，使得cpu的利用率大大提升，cup的性能大幅度的被利用。  goroutine 调度器GPM模型 G  G 就是goroutine协程  type g struct { // Stack parameters. // stack describes the actual stack memory: [stack.lo, stack.hi). // stackguard0 is the stack pointer compared in the Go stack growth prologue. // It is stack.lo+StackGuard normally, but can be StackPreempt to trigger a preemption. // stackguard1 is the stack pointer compared in the C stack growth prologue. // It is stack.lo+StackGuard on g0 and gsignal stacks. // It is ~0 on other goroutine stacks, to trigger a call to morestackc (and crash). // 记录该goroutine使用的栈 stack stack // offset known to runtime/cgo //下面两个成员用于栈溢出检查，实现栈的自动伸缩，抢占调度也会用到stackguard0 stackguard0 uintptr // offset known to liblink stackguard1 uintptr // offset known to liblink _panic *_panic // innermost panic - offset known to liblink _defer *_defer // innermost defer // 此goroutine正在被哪个工作线程执行 m *m // current m; offset known to arm liblink //这个字段跟调度切换有关，G切换时用来保存上下文，保存什么，看下面gobuf结构体 sched gobuf syscallsp uintptr // if status==Gsyscall, syscallsp = sched.sp to use during gc syscallpc uintptr // if status==Gsyscall, syscallpc = sched.pc to use during gc stktopsp uintptr // expected sp at top of stack, to check in traceback param unsafe.Pointer // passed parameter on wakeup，wakeup唤醒时传递的参数 // 状态Gidle,Grunnable,Grunning,Gsyscall,Gwaiting,Gdead atomicstatus uint32 stackLock uint32 // sigprof/scang lock; TODO: fold in to atomicstatus goid int64 //schedlink字段指向全局运行队列中的下一个g， //所有位于全局运行队列中的g形成一个链表 schedlink guintptr waitsince int64 // approx time when the g become blocked waitreason waitReason // if status==Gwaiting，g被阻塞的原因 //抢占信号，stackguard0 = stackpreempt，如果需要抢占调度，设置preempt为true preempt bool // preemption signal, duplicates stackguard0 = stackpreempt paniconfault bool // panic (instead of crash) on unexpected fault address preemptscan bool // preempted g does scan for gc gcscandone bool // g has scanned stack; protected by _Gscan bit in status gcscanvalid bool // false at start of gc cycle, true if G has not run since last scan; TODO: remove? throwsplit bool // must not split stack raceignore int8 // ignore race detection events sysblocktraced bool // StartTrace has emitted EvGoInSyscall about this goroutine sysexitticks int64 // cputicks when syscall has returned (for tracing) traceseq uint64 // trace event sequencer tracelastp puintptr // last P emitted an event for this goroutine // 如果调用了 LockOsThread，那么这个 g 会绑定到某个 m 上 lockedm muintptr sig uint32 writebuf []byte sigcode0 uintptr sigcode1 uintptr sigpc uintptr // 创建这个goroutine的go表达式的pc gopc uintptr // pc of go statement that created this goroutine ancestors *[]ancestorInfo // ancestor information goroutine(s) that created this goroutine (only used if debug.tracebackancestors) startpc uintptr // pc of goroutine function racectx uintptr waiting *sudog // sudog structures this g is waiting on (that have a valid elem ptr); in lock order cgoCtxt []uintptr // cgo traceback context labels unsafe.Pointer // profiler labels timer *timer // cached timer for time.Sleep,为 time.Sleep 缓存的计时器 selectDone uint32 // are we participating in a select and did someone win the race? // Per-G GC state // gcAssistBytes is this G's GC assist credit in terms of // bytes allocated. If this is positive, then the G has credit // to allocate gcAssistBytes bytes without assisting. If this // is negative, then the G must correct this by performing // scan work. We track this in bytes to make it fast to update // and check for debt in the malloc hot path. The assist ratio // determines how this corresponds to scan work debt. gcAssistBytes int64 }  保存着goroutine所有信息以及栈信息，gobuf结构体：cpu里的寄存器信息  P processor处理器  调度协程G和线程M的关联  P 的结构体如下： type p struct { //allp中的索引 id int32 //p的状态 status uint32 // one of pidle/prunning/... link puintptr schedtick uint32 // incremented on every scheduler call-\u0026gt;每次scheduler调用+1 syscalltick uint32 // incremented on every system call-\u0026gt;每次系统调用+1 sysmontick sysmontick // last tick observed by sysmon //指向绑定的 m，如果 p 是 idle 的话，那这个指针是 nil m muintptr // back-link to associated m (nil if idle) mcache *mcache raceprocctx uintptr //不同大小可用defer结构池 deferpool [5][]*_defer // pool of available defer structs of different sizes (see panic.go) deferpoolbuf [5][32]*_defer // Cache of goroutine ids, amortizes accesses to runtime·sched.goidgen. goidcache uint64 goidcacheend uint64 //本地运行队列，可以无锁访问 // Queue of runnable goroutines. Accessed without lock. runqhead uint32 //队列头 runqtail uint32 //队列尾 //数组实现的循环队列 runq [256]guintptr // runnext, if non-nil, is a runnable G that was ready'd by // the current G and should be run next instead of what's in // runq if there's time remaining in the running G's time // slice. It will inherit the time left in the current time // slice. If a set of goroutines is locked in a // communicate-and-wait pattern, this schedules that set as a // unit and eliminates the (potentially large) scheduling // latency that otherwise arises from adding the ready'd // goroutines to the end of the run queue. // runnext 非空时，代表的是一个 runnable 状态的 G， //这个 G 被 当前 G 修改为 ready 状态，相比 runq 中的 G 有更高的优先级。 //如果当前 G 还有剩余的可用时间，那么就应该运行这个 G //运行之后，该 G 会继承当前 G 的剩余时间 runnext guintptr // Available G's (status == Gdead) //空闲的g gFree struct { gList n int32 } sudogcache []*sudog sudogbuf [128]*sudog tracebuf traceBufPtr // traceSweep indicates the sweep events should be traced. // This is used to defer the sweep start event until a span // has actually been swept. traceSweep bool // traceSwept and traceReclaimed track the number of bytes // swept and reclaimed by sweeping in the current sweep loop. traceSwept, traceReclaimed uintptr palloc persistentAlloc // per-P to avoid mutex _ uint32 // Alignment for atomic fields below // Per-P GC state gcAssistTime int64 // Nanoseconds in assistAlloc gcFractionalMarkTime int64 // Nanoseconds in fractional mark worker (atomic) gcBgMarkWorker guintptr // (atomic) gcMarkWorkerMode gcMarkWorkerMode // gcMarkWorkerStartTime is the nanotime() at which this mark // worker started. gcMarkWorkerStartTime int64 // gcw is this P's GC work buffer cache. The work buffer is // filled by write barriers, drained by mutator assists, and // disposed on certain GC state transitions. gcw gcWork // wbBuf is this P's GC write barrier buffer. // // TODO: Consider caching this in the running G. wbBuf wbBuf runSafePointFn uint32 // if 1, run sched.safePointFn at next safe point pad cpu.CacheLinePad }  记录着P的信息，以及G的状态等。同时P是有着本地队列，存放着带待运行的G,本地队列不能超过256个。 P的数量：是由环境变量 $GOMAXPROCS 或者是由 runtime 的方法 GOMAXPROCS() 决定。在程序启动式创建，并保存在数组中，最多有 GOMAXPROCS(可配置) 个  M 是内核态线程的抽象  主要的工作执行协程G或者在调度G到P中  M的结构体如下： type m struct { // 系统管理的一个g，执行调度代码时使用的。比如执行用户的goroutine时，就需要把把用户 // 的栈信息换到内核线程的栈，以便能够执行用户goroutine g0 *g // goroutine with scheduling stack morebuf gobuf // gobuf arg to morestack divmod uint32 // div/mod denominator for arm - known to liblink // Fields not known to debuggers. procid uint64 // for debuggers, but offset not hard-coded //处理signal的 g gsignal *g // signal-handling g goSigStack gsignalStack // Go-allocated signal handling stack sigmask sigset // storage for saved signal mask //线程的本地存储TLS，这里就是为什么OS线程能运行M关键地方 tls [6]uintptr // thread-local storage (for x86 extern register) //go 关键字运行的函数 mstartfn func() //当前运行的用户goroutine的g结构体对象 curg *g // current running goroutine caughtsig guintptr // goroutine running during fatal signal //当前工作线程绑定的P，如果没有就为nil p puintptr // attached p for executing go code (nil if not executing go code) //暂存与当前M潜在关联的P nextp puintptr //M之前调用的P oldp puintptr // the p that was attached before executing a syscall id int64 mallocing int32 throwing int32 //当前M是否关闭抢占式调度 preemptoff string // if != \u0026quot;\u0026quot;, keep curg running on this m locks int32 dying int32 profilehz int32 //M的自旋状态，为true时M处于自旋状态，正在从其他线程偷G; 为false，休眠状态 spinning bool // m is out of work and is actively looking for work blocked bool // m is blocked on a note newSigstack bool // minit on C thread called sigaltstack printlock int8 incgo bool // m is executing a cgo call freeWait uint32 // if == 0, safe to free g0 and delete m (atomic) fastrand [2]uint32 needextram bool traceback uint8 ncgocall uint64 // number of cgo calls in total ncgo int32 // number of cgo calls currently in progress cgoCallersUse uint32 // if non-zero, cgoCallers in use temporarily cgoCallers *cgoCallers // cgo traceback if crashing in cgo call //没有goroutine运行时，工作线程睡眠 //通过这个来唤醒工作线程 park note // 休眠锁 //记录所有工作线程的链表 alllink *m // on allm schedlink muintptr //当前线程内存分配的本地缓存 mcache *mcache //当前M锁定的G， lockedg guintptr createstack [32]uintptr // stack that created this thread. lockedExt uint32 // tracking for external LockOSThread lockedInt uint32 // tracking for internal lockOSThread nextwaitm muintptr // next m waiting for lock waitunlockf func(*g, unsafe.Pointer) bool waitlock unsafe.Pointer waittraceev byte waittraceskip int startingtrace bool syscalltick uint32 //操作系统线程id thread uintptr // thread handle freelink *m // on sched.freem // these are here because they are too large to be on the stack // of low-level NOSPLIT functions. libcall libcall libcallpc uintptr // for cpu profiler libcallsp uintptr libcallg guintptr syscall libcall // stores syscall parameters on windows vdsoSP uintptr // SP for traceback while in VDSO call (0 if not in call) vdsoPC uintptr // PC for traceback while in VDSO call dlogPerM mOS }  记录着M的线程的信息，包括一些P,G以及信号和自旋锁等信息 m 数量：可以通过SetMaxThreads函数，设置 M 的最大数量，默认为10000(sched.maxmcount = 10000)，和P一样在程序启动时创建。  全局队列（gQueue）  P的本地队列可以存放着不超过256个待执行的G,P是有限的，当G过多时，即当P本地队列存放不下时，就需要将G存放在全局队列中。  全局队列结构如下： type gQueue struct { head guintptr //队列头 tail guintptr //队列尾 } G、P、M、gQueue关系  P与M没有数量关系，当一个M处于阻塞时，P先找空闲M,没有空闲的M就创建新的M G优先存放在P本地队列中，当P中G满时，会将P中前一半G存放在全局中。当P空闲时时，会从全局中拿取G放在本地队列。全局没有G时，会从其P的本地队列中拿取一半到本地队列。 关系如图所示：\n\r  创建goroutine newproc()函数  goroutine 是由函数newproc函数进行创建的，newproc源码如下  // 参数：协程函数的参数占的字节数和协程入口函数的funcval指针 func newproc(siz int32, fn *funcval) { // 获得协程参数的地址= fn函数地址+偏移值 argp := add(unsafe.Pointer(\u0026amp;fn), sys.PtrSize) gp := getg() // 获得当前G的指针 //调用者的pc，也就是执行完此函数返回调用者时的下一条指令地址 pc := getcallerpc() // 切换到（系统栈）g0栈中 systemstack(func() { //执行调用newproc1()函数执行创建协程 newg := newproc1(fn, argp, siz, gp, pc) _p_ := getg().m.p.ptr() // 把当前的G存放在runq队列中 runqput(_p_, newg, true) // 如果当前由空闲的P,没有睡眠的M,主协程开始运行时 if mainStarted { wakep() // 创建m,并设置为活跃状态 } }) }  在newproc函数中为什么要切换在g0栈中执行呢？是因为newproc1()函数不支持栈增长，协程的栈空间小(几KB)，为了防止运行协程函数时栈溢出，需要在g0的栈上运行，g0是分配在线程的栈空间(4MB)上。g0的栈空间很大，运行协程函数时栈不溢出。  newproc1()函数  newproc1()是创建协程 源码如下：  参数：协程入口、参数首地址、参数大小、父协程指针、返回地址 func newproc1(fn *funcval, argp unsafe.Pointer, narg int32, callergp *g, callerpc uintptr) *g { _g_ := getg() // 获得当前的G if fn == nil { _g_.m.throwing = -1 // do not dump full stacks throw(\u0026quot;go of nil func value\u0026quot;) } // 为了保证数据一致性会禁止当前m被抢占 acquirem() // disable preemption because it can be holding p in a local var siz := narg siz = (siz + 7) \u0026amp;^ 7 // We could allocate a larger initial stack if necessary. // Not worth it: this is almost always an error. // 4*sizeof(uintreg): extra space added below // sizeof(uintreg): caller's LR (arm) or return address (x86, in gostartcall). if siz \u0026gt;= _StackMin-4*sys.RegSize-sys.RegSize { throw(\u0026quot;newproc: function arguments too large for new goroutine\u0026quot;) } _p_ := _g_.m.p.ptr() // 尝试获取一个空闲的G,如果没有空闲的G,就会创建新的G,分配栈空间,并添加到全局allgs中 newg := gfget(_p_) // 如果没有空闲的G if newg == nil { // 就会创建新的G,分配栈空间大小为最小的2KB newg = malg(_StackMin) // 设置状态为等待 casgstatus(newg, _Gidle, _Gdead) //并添加到全局allgs中 allgadd(newg) // publishes with a g-\u0026gt;status of Gdead so GC scanner doesn't look at uninitialized stack. } if newg.stack.hi == 0 { throw(\u0026quot;newproc1: newg missing stack\u0026quot;) } if readgstatus(newg) != _Gdead { throw(\u0026quot;newproc1: new g is not Gdead\u0026quot;) } totalSize := 4*sys.RegSize + uintptr(siz) + sys.MinFrameSize // extra space in case of reads slightly beyond frame totalSize += -totalSize \u0026amp; (sys.SpAlign - 1) // align to spAlign sp := newg.stack.hi - totalSize spArg := sp if usesLR { // caller's LR *(*uintptr)(unsafe.Pointer(sp)) = 0 prepGoExitFrame(sp) spArg += sys.MinFrameSize } if narg \u0026gt; 0 { // 如果协程入口函数由参数，会将参数移动在协程栈中 memmove(unsafe.Pointer(spArg), argp, uintptr(narg)) // This is a stack-to-stack copy. If write barriers // are enabled and the source stack is grey (the // destination is always black), then perform a // barrier copy. We do this *after* the memmove // because the destination stack may have garbage on // it. if writeBarrier.needed \u0026amp;\u0026amp; !_g_.m.curg.gcscandone { f := findfunc(fn.fn) stkmap := (*stackmap)(funcdata(f, _FUNCDATA_ArgsPointerMaps)) if stkmap.nbit \u0026gt; 0 { // We're in the prologue, so it's always stack map index 0. bv := stackmapdata(stkmap, 0) bulkBarrierBitmap(spArg, spArg, uintptr(bv.n)*sys.PtrSize, 0, bv.bytedata) } } } // 初始化newg.sched调度相关的信息 memclrNoHeapPointers(unsafe.Pointer(\u0026amp;newg.sched), unsafe.Sizeof(newg.sched)) newg.sched.sp = sp //设置为协程栈指针 newg.stktopsp = sp // 设置为指向协程入口函数的入口，当协程调度执行时，运行协程函数 newg.sched.pc = funcPC(goexit) + sys.PCQuantum // +PCQuantum so that previous instruction is in same function newg.sched.g = guintptr(unsafe.Pointer(newg)) gostartcallfn(\u0026amp;newg.sched, fn) // 设置为父协程调用newproc函数结束后的返回地址 newg.gopc = callerpc newg.ancestors = saveAncestors(callergp) // 设置startpc为协程入孔函数的起始地址 newg.startpc = fn.fn if _g_.m.curg != nil { newg.labels = _g_.m.curg.labels } if isSystemGoroutine(newg, false) { atomic.Xadd(\u0026amp;sched.ngsys, +1) } // 设置协程为运行状态 casgstatus(newg, _Gdead, _Grunnable) if _p_.goidcache == _p_.goidcacheend { // Sched.goidgen is the last allocated id, // this batch must be [sched.goidgen+1, sched.goidgen+GoidCacheBatch]. // At startup sched.goidgen=0, so main goroutine receives goid=1. _p_.goidcache = atomic.Xadd64(\u0026amp;sched.goidgen, _GoidCacheBatch) _p_.goidcache -= _GoidCacheBatch - 1 _p_.goidcacheend = _p_.goidcache + _GoidCacheBatch } // 给协程赋予一个唯一的goid newg.goid = int64(_p_.goidcache) _p_.goidcache++ if raceenabled { newg.racectx = racegostart(callerpc) } if trace.enabled { traceGoCreate(newg, newg.startpc) } // 允许当前m被抢占 releasem(_g_.m) return newg } 图示如下\n\r\n 总结goroutine创建过程   为了保证数据一致性会禁止当前m被抢占 尝试获取一个空闲的G,如果没有空闲的G,就会创建新的G,分配栈空间,状态为等待并添加到全局allgs中 如果协程入口函数由参数，会将参数移动在协程栈中 初始化newg.sched调度相关的信息，设置状态运行 得到唯一的goid, 并添加到runq队列中 如果当前有空闲的P,没有睡眠的M,并且主协程开始运行时，就会创建新的活跃的M 当g运行结束后，设置允许当前m被抢占  goroutine的让出与恢复、调度、抢占、监控 goroutine 让出与恢复  协程的让出是由函数gopark()执行的  源码如下： func gopark(unlockf func(*g, unsafe.Pointer) bool, lock unsafe.Pointer, reason waitReason, traceEv byte, traceskip int) { if reason != waitReasonSleep { checkTimeouts() // timeouts may expire while two goroutines keep the scheduler busy } // 禁止当前m被抢占 mp := acquirem() gp := mp.curg status := readgstatus(gp) // 判断协程的是否在运行状态 if status != _Grunning \u0026amp;\u0026amp; status != _Gscanrunning { throw(\u0026quot;gopark: bad g status\u0026quot;) } mp.waitlock = lock mp.waitunlockf = unlockf gp.waitreason = reason mp.waittraceev = traceEv mp.waittraceskip = traceskip // 解除对m的抢占 releasem(mp) // can't do anything that might move the G between Ms here. // 不能做任何可能在 Ms 之间移动 G 的事情。 // 保存协程，切换在go mcall(park_m) } func park_m(gp *g) { _g_ := getg() if trace.enabled { traceGoPark(_g_.m.waittraceev, _g_.m.waittraceskip) } //更改协程由运行状态到等待状态 casgstatus(gp, _Grunning, _Gwaiting) dropg() \u0026quot;\u0026quot;\u0026quot; func dropg() { _g_ := getg() // 把m当前执行的置为nil(m不在运行这个当前写协程，协程就挂起了) setMNoWB(\u0026amp;_g_.m.curg.m, nil) setGNoWB(\u0026amp;_g_.m.curg, nil) } \u0026quot;\u0026quot;\u0026quot; if fn := _g_.m.waitunlockf; fn != nil { ok := fn(gp, _g_.m.waitlock) _g_.m.waitunlockf = nil _g_.m.waitlock = nil if !ok { if trace.enabled { traceGoUnpark(gp, 2) } casgstatus(gp, _Gwaiting, _Grunnable) execute(gp, true) // Schedule it back, never returns. } } schedule() // 寻找下一个G } //在G中由定时调用回调函数f type timer struct { // If this timer is on a heap, which P's heap it is on. // puintptr rather than *p to match uintptr in the versions // of this struct defined in other packages. pp puintptr // Timer wakes up at when, and then at when+period, ... (period \u0026gt; 0 only) // each time calling f(arg, now) in the timer goroutine, so f must be // a well-behaved function and not block. // // when must be positive on an active timer. when int64 period int64 f func(interface{}, uintptr) arg interface{} seq uintptr // What to set the when field to in timerModifiedXX status. nextwhen int64 // The status field holds one of the values below. status uint32 } // Mark gp ready to run. // 将等待协程状态置为运行的状态 func ready(gp *g, traceskip int, next bool) { if trace.enabled { traceGoUnpark(gp, traceskip) } status := readgstatus(gp) // Mark runnable. _g_ := getg() //// 禁止当前m被抢占 mp := acquirem() // disable preemption because it can be holding p in a local var if status\u0026amp;^_Gscan != _Gwaiting { dumpgstatus(gp) throw(\u0026quot;bad g-\u0026gt;status in ready\u0026quot;) } // status is Gwaiting or Gscanwaiting, make Grunnable and put on runq casgstatus(gp, _Gwaiting, _Grunnable) // 把协程等待的转态置为可运行状态 runqput(_g_.m.p.ptr(), gp, next) // 添加在运行队列中 wakep()// 如果没有可执行的M,就创建新的m releasem(mp) // 释放当前的m } 如图所示 \r\n 总结   gopark()是让出函数，禁止当前m被抢占，判断当前的协程状态是否为运行状态。 dropg()让当前的m不在执行当前的G,修改当前g的状态为等待(协程挂起)，调用schedule() 寻找下一个可执行G timers 等待的g中数据结构，定时调用回调函数f，将g置为了运行状态 ready()函数是将唤醒等待G,将G的状态更改为可运行状态，并添加在运行的队列中m，如果没有可执行的M,就创建新的m  goroutine 监控  使用checkTimers()检查到时间运行的唤醒g  源码如下 checkTimers(pp *p, now int64) (rnow, pollUntil int64, ran bool) { // If it's not yet time for the first timer, or the first adjusted // timer, then there is nothing to do. next := int64(atomic.Load64(\u0026amp;pp.timer0When)) nextAdj := int64(atomic.Load64(\u0026amp;pp.timerModifiedEarliest)) if next == 0 || (nextAdj != 0 \u0026amp;\u0026amp; nextAdj \u0026lt; next) { next = nextAdj } if next == 0 { // No timers to run or adjust. return now, 0, false } if now == 0 { now = nanotime() } if now \u0026lt; next { // Next timer is not ready to run, but keep going // if we would clear deleted timers. // This corresponds to the condition below where // we decide whether to call clearDeletedTimers. if pp != getg().m.p.ptr() || int(atomic.Load(\u0026amp;pp.deletedTimers)) \u0026lt;= int(atomic.Load(\u0026amp;pp.numTimers)/4) { return now, next, false } } lock(\u0026amp;pp.timersLock) if len(pp.timers) \u0026gt; 0 { adjusttimers(pp, now) for len(pp.timers) \u0026gt; 0 { // Note that runtimer may temporarily unlock // pp.timersLock. if tw := runtimer(pp, now); tw != 0 { if tw \u0026gt; 0 { pollUntil = tw } break } ran = true } } // If this is the local P, and there are a lot of deleted timers, // clear them out. We only do this for the local P to reduce // lock contention on timersLock. if pp == getg().m.p.ptr() \u0026amp;\u0026amp; int(atomic.Load(\u0026amp;pp.deletedTimers)) \u0026gt; len(pp.timers)/4 { clearDeletedTimers(pp) } unlock(\u0026amp;pp.timersLock) return now, pollUntil, ran }  协程的监控是由专门的监控协程程来运行，监控协程是由主协程创建而来 ，监控协程与gpm中的协程不一样，它不是由gpm进行调度，当然了也不需要P, 监控timer，并按需调整g的休眠时间，如果没有可执行的M,就创建新的m执行被唤醒的G, 确保被唤醒g被执行。 如图 \r  goroutine 抢占  对运行过长的g进行抢占，即当g运行时间超过运行阈值的g强制让出m 运行时间是由P的结构syscalltick、schedtick、timer0When等记录。 通过栈增长时：当stackguard = stackPreempt,不执行栈增长，而是执行协程调度, 这样就让协程让出栈。 这种抢占依赖栈增长，有缺陷。所以有asyncPreempt通过信号方式进行异步抢占\n如图所示 \r  goroutine 调度  调用schedule()函数进行协程的调度  源码如下： func schedule() { _g_ := getg() // 获得当前的G if _g_.m.locks != 0 { throw(\u0026quot;schedule: holding locks\u0026quot;) } // 判断当前的M和当前的G是否绑定 if _g_.m.lockedg != 0 { // 如果当前的M绑定G,就阻塞m(休眠M) stoplockedm() execute(_g_.m.lockedg.ptr(), false) // Never returns. } // We should not schedule away from a g that is executing a cgo call, // since the cgo call is using the m's g0 stack. if _g_.m.incgo { throw(\u0026quot;schedule: in cgo\u0026quot;) } top: pp := _g_.m.p.ptr() pp.preempt = false // 判断Gc是否在等待执行 if sched.gcwaiting != 0 { //是在等待执行，先执行gc，执行完在执行后续操作 gcstopm() goto top } if pp.runSafePointFn != 0 { runSafePointFn() } // Sanity check: if we are spinning, the run queue should be empty. // Check this before calling checkTimers, as that might call // goready to put a ready goroutine on the local run queue. if _g_.m.spinning \u0026amp;\u0026amp; (pp.runnext != 0 || pp.runqhead != pp.runqtail) { throw(\u0026quot;schedule: spinning with local work\u0026quot;) } //检查是否有要被执行的Timer checkTimers(pp, 0) var gp *g var inheritTime bool // Normal goroutines will check for need to wakeP in ready, // but GCworkers and tracereaders will not, so the check must // be done here instead. // 普通的 goroutine 会检查是否需要在准备好时唤醒， // 但 GCworkers 和跟踪读取器不会，所以检查必须 // 在这里完成。 tryWakeP := false if trace.enabled || trace.shutdown { gp = traceReader() if gp != nil { casgstatus(gp, _Gwaiting, _Grunnable) traceGoUnpark(gp, 0) tryWakeP = true } } if gp == nil \u0026amp;\u0026amp; gcBlackenEnabled != 0 { gp = gcController.findRunnableGCWorker(_g_.m.p.ptr()) tryWakeP = tryWakeP || gp != nil } if gp == nil { // Check the global runnable queue once in a while to ensure fairness. // Otherwise two goroutines can completely occupy the local runqueue // by constantly respawning each other. / 每隔一段时间检查一下全局可运行队列以确保公平。 // 否则两个 goroutine 可以完全占用本地运行队列 // 通过不断相互重生。 // 有%61的概率把G从全局运行队列中搬移到本地可运行队列，保障本地可运行队列 有G运行，全局队列也能放在本都队列中 if _g_.m.p.ptr().schedtick%61 == 0 \u0026amp;\u0026amp; sched.runqsize \u0026gt; 0 { lock(\u0026amp;sched.lock) gp = globrunqget(_g_.m.p.ptr(), 1) unlock(\u0026amp;sched.lock) } } if gp == nil { // 没有待运行的G 就现在本地可运行队列查找 gp, inheritTime = runqget(_g_.m.p.ptr()) // We can see gp != nil here even if the M is spinning, // if checkTimers added a local goroutine via goready. } if gp == nil { // 本地队列没有，就调用findrunnable()，直到有待执行的g才返回(先在本地 运行队列，全局队列、等待的io, 其他的P) gp, inheritTime = findrunnable() // blocks until work is available } // This thread is going to run a goroutine and is not spinning anymore, // so if it was marked as spinning we need to reset it now and potentially // start a new spinning M. if _g_.m.spinning { resetspinning() } if sched.disable.user \u0026amp;\u0026amp; !schedEnabled(gp) { // Scheduling of this goroutine is disabled. Put it on // the list of pending runnable goroutines for when we // re-enable user scheduling and look again. lock(\u0026amp;sched.lock) if schedEnabled(gp) { // Something re-enabled scheduling while we // were acquiring the lock. unlock(\u0026amp;sched.lock) } else { sched.disable.runnable.pushBack(gp) sched.disable.n++ unlock(\u0026amp;sched.lock) goto top } } // If about to schedule a not-normal goroutine (a GCworker or tracereader), // wake a P if there is one. if tryWakeP { wakep() } // 判断获得的G有没有绑定的M,有就阻塞g, 再次进行调度 if gp.lockedm != 0 { // Hands off own p to the locked m, // then blocks waiting for a new p. startlockedm(gp) goto top } // 使用execute函数让m执行g execute(gp, inheritTime) } 如图所示 \r\n 总结   判断当前的M和当前的G是否绑定，如果当前的M绑定G,就阻塞m(休眠M) 判断Gc是否在等待执行，是在等待执行，先执行gc，执行完在执行后续操作 检查是否有要被执行的Timer 普通的 goroutine 会检查是否需要在准备好时唤醒，但 GCworkers 和跟踪读取器不会，所以检查必须 有%61的概率把G从全局运行队列中搬移到本地可运行队列，保障本地可运行队列有G运行，全局队列也能放在本都队列中 没有待运行的G就现在本地可运行队列查找，本地队列没有，就调用findrunnable()，直到有待执行的g才返回(先在本地 运行队列，全局队列、等待的io, 其他的P中分配G) 判断获得的G有没有绑定的M,有就阻塞g, 再次进行调度 使用execute函数让m执行g,待运行g绑定m,调用gogo(\u0026amp;gp.sched)协程的现场恢复等  调度器的设计策略  减少线程的创建与销毁cup的开销，GPM是线程的复用。即当没有 可运行的G时，将M休眠,P空闲。当有可执行G是找空闲的P，在将M唤醒，执行G，直到 main.main 退出，runtime.main 执行 Defer 和 Panic 处理，或调用 runtime.exit 退出程序 work stealing 机制：当本线程无可运行的 G 时，尝试从其他线程绑定的 P 偷取 G，而不是销毁线程。 hand off 机制：\n1.当本线程因为 G 进行系统调用阻塞时，线程释放绑定的 P，把 P 转移给其他空闲的线程执行。\n2.利用并行：GOMAXPROCS 设置 P 的数量，最多有 GOMAXPROCS 个线程分布在多个 CPU 上同时运行。GOMAXPROCS 也限制了并发的程度，比如 GOMAXPROCS = 核数/2，则最多利用了一半的 CPU 核进行并行。 3.抢占：在 coroutine 中要等待一个协程主动让出 CPU 才执行下一个协程，在 Go 中，一个 goroutine 最多占用 CPU 10ms，防止其他 goroutine 被饿死，这就是 goroutine 不同于 coroutine 的一个地方。\n4.全局 G 队列：在新的调度器中依然有全局 G 队列，但功能已经被弱化了，当 M 执行 work stealing 从其他 P 偷不到 G 时，它可以从全局 G 队列获取 G。 调度如图所示\n\r  参考文献 1、https://www.jianshu.com/p/fa696563c38a 2.https://www.zhihu.com/people/kylin-lab\n","date":"2021-10-06T22:00:38+08:00","image":"https://zcj-git520.github.io/p/go-goroutine%E4%B8%8Egmp%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3/1_huda718813a1636432ee91da972c85e460_240629_120x120_fill_box_smart1_3.png","permalink":"https://zcj-git520.github.io/p/go-goroutine%E4%B8%8Egmp%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3/","title":"go goroutine与gmp模型的深入理解"},{"content":"理解进程与线程 进程  进程是程序一次动态执行过程、进程是操作系统分配资源(内存、io资源、cpu等)和资源调度的基本单位。程序是指令、数据及其组织形式的描述，进程是程序的实体。 进程是由 进程控制块PCB、相关程序段和该程序段进行操作的数据结构集三个部分组成。 进程的五中状态：创建、就绪、运行、阻塞、终止 五种状态转换如图所示： \r  线程  线程是cup调度和分配的基本单位也是cup执行的最小单位, 有独立的栈空间，共享堆空间。  进程与线程的关系  一个进程可以创建和撤销多个线程， 一个进程必须有一个线程(主线程), 线程共享进程所有资源，进程是线程的容器，关系如图所示：\n\r  并发与并行 并发  并发：多进程(线程)程序在一个核cup串行运行，当一个进程(线程)阻塞的时候，切换到另外等待执行的进程(线程) 如图\n\r  并行  并行：多线程程序在多核cup并行运行，如图\n\r  用户态和内核态(用户空间和内核空间) 特权级划分  cpu一共有0～4四个特权级，R0级最高，R3级最低。用户态指的是：程序运行在R3级以上，通常在应用程序中运行，内核态是指：程序运行在R0级以上，通常在内核中运行。一般来说，我们写的应用程序就是运行在R3级衣以上。  3中种用户态与内核态的切换   系统调用：用户态进程通过系统调用申请使用操作系统提供的服务程序完成工作，比如前例中fork()实际上就是执行了一个创建新进程的系统调用。而系统调用的机制其核心还是使用了操作系统为用户特别开放的一个中断来实现，例如Linux的int 80h中断。\n  异常：当CPU在执行运行在用户态下的程序时，发生了某些事先不可知的异常，这时会触发由当前运行进程切换到处理此异常的内核相关程序中，也就转到了内核态，比如缺页异常。\n  外围设备的中断： 当外围设备完成用户请求的操作后，会向CPU发出相应的中断信号，这时CPU会暂停执行下一条即将要执行的指令转而去执行与中断信号对应的处理程序，如果先前执行的指令是用户态下的程序，那么这个转换的过程自然也就发生了由用户态到内核态的切换。比如硬盘读写操作的完成，系统会切换到硬盘读写的中断处理程序中执行后续操作等。\n  用户态与内核态结构如图：\n\r\n  用户态与内核态的切换是需要开销\n  来源：linux用户态和内核态理解(https://www.cnblogs.com/weifeng1463/p/11660260.html)\n  进程与线程用户态到内核态的开销   多进程(线程)可以提高cpu的利用率，减少程序阻塞带来cpu闲置的情况，也就是提升cpu的运行时间片，但是过多的创建进程(线程)也会花费额外的cpu时间片进行进程(线程)的花销。进程的创建、就绪、运行、阻塞、终止，这些都会带来cup花销。例如在32位的操作系统中创建一个进程需要开辟4GB的虚拟内存空间，创建一个线程需要占用约4MB的内存。\n\r\n  进程(线程)的调度也会带来cup的花销。cup进程(线程)的调度就是进程(线程)切换，进程(线程)的切换就会进行线程在内核态的调度。cup切换的内核态的线程，不操作用户态的线程，用户态线程通过系统调用触发内核线程。\n  为减少cpu内核态线程之间的切换，操作系统中使用(用户态进程(线程):内核态进程(线程))1:1，用户态直接通过系统，直接与内核态的线程一一对应。如图\n\r\n  用户态一个进程(线程)对应一个内核态的进程(线程)是减少了内核态中进程(线程)切换的花销，但是也增加了内核态中进程(线程)创建的开销。减少内核态中进程(线程)切换与创建带来的开销，操作系统中使用(用户态进程(线程):内核态进程(线程))N:1，减少内核态中进程(线程)的创建，同时在用户态进行线程的之间的切换，不牵连内核态线程的切换，减少cup的花销。 \r\n  虽然N:1减少内核态中进程(线程)切换与创建带来的开销，但是当用户态的进程(线程)阻塞时，其他进程(线程)就只能等待，这造成与单线程一样的问题。操作系统结合1:1和n:1模型的有点形成n:m模型，内核态中进程(线程)进入阻塞状态时， 用户态的进程(线程)切换另一个内核态中的线程。\n\r\n  协程  协程和线程一样有独立的栈空间，共享堆空间，是用户级的线程，是有用户自己调度。一个线程可以创建多个协程，协程是轻量级的线程。创建一个协程只需要占用4~5kB的虚拟内存，创建协程的开销相比进程与线程低太多了。\n\r  参考文献 1、https://zhuanlan.zhihu.com/p/337978321 2、https://www.jianshu.com/p/fa696563c38a\n","date":"2021-09-28T22:00:38+08:00","image":"https://zcj-git520.github.io/p/c/c/10_hu5bb3a4baa791897c79c1df91e792ef0e_243913_120x120_fill_box_smart1_3.png","permalink":"https://zcj-git520.github.io/p/c/c-/","title":"进程、线程、协程"},{"content":"goland 基础之map map的内部结构  go map是使用的哈希表构建的 map的结构可分为：hmap的结构体和bmap(桶)，hmap结构体记录这map的基础信息(包括map存储个数， 桶的个数，hash种子，桶的数据，扩容时旧桶的数据以及迁移个数（map扩容不是一次性迁移完）) 源码如下    定义hmap的结构： type hmap struct { // Note: the format of the hmap is also encoded in cmd/compile/internal/gc/reflect.go. // Make sure this stays in sync with the compiler's definition. // map 存储元素的计数 count int // # live cells == size of map. Must be first (used by len() builtin) flags uint8 // map的状态标识，桶是否在增改，扩容或者缩容 //桶的个数/采用的与运算法计算桶的个数，桶的个数为2的整数次幂 B uint8 // log_2 of # of buckets (can hold up to loadFactor * 2^B items) //溢出的桶的数量的近似值 noverflow uint16 // approximate number of overflow buckets; see incrnoverflow for details hash0 uint32 // hash seed //指向桶数据的指针 buckets unsafe.Pointer // array of 2^B Buckets. may be nil if count==0. // 指向旧桶数据的指针 oldbuckets unsafe.Pointer // previous bucket array of half the size, non-nil only when growing //扩容计数 nevacuate uintptr // progress counter for evacuation (buckets less than this have been evacuated) // 保存溢出桶的链表和未使用的溢出桶数组的首地址 extra *mapextra // optional fields } // 桶的实现结构 type bmap struct { // 当前版本bucketCnt的值是8，一个桶最多存储8个key-value对 tophash [bucketCnt]uint8 }  bmap存储结构如图所示\n\r 前8个是hash值，8个key和8个value、后面是溢出桶的指针 溢出桶是减少map扩容次数，溢出桶的结构与bmap桶的结构一样的 溢出桶的基础结构：    type mapextra struct { // If both key and elem do not contain pointers and are inline, then we mark bucket // type as containing no pointers. This avoids scanning such maps. // However, bmap.overflow is a pointer. In order to keep overflow buckets // alive, we store pointers to all overflow buckets in hmap.extra.overflow and hmap.extra.oldoverflow. // overflow and oldoverflow are only used if key and elem do not contain pointers. // overflow contains overflow buckets for hmap.buckets. // oldoverflow contains overflow buckets for hmap.oldbuckets. // The indirection allows to store a pointer to the slice in hiter. overflow *[]*bmap //记录已经被使用的溢出桶 oldoverflow *[]*bmap // 扩容阶段旧的溢出桶 // nextOverflow holds a pointer to a free overflow bucket. nextOverflow *bmap //指向下一个空闲的溢出桶 }  当桶的个数大于2的4次方时就会使用溢出桶源码如下  func makeBucketArray(t *maptype, b uint8, dirtyalloc unsafe.Pointer) (buckets unsafe.Pointer, nextOverflow *bmap) { // 桶的个数 base := bucketShift(b) nbuckets := base // For small b, overflow buckets are unlikely. // Avoid the overhead of the calculation. if b \u0026gt;= 4 { // 使用溢出桶 // Add on the estimated number of overflow buckets // required to insert the median number of elements // used with this value of b. nbuckets += bucketShift(b - 4)//计算溢出桶的数量和不是溢出桶的数量的和 sz := t.bucket.size * nbuckets up := roundupsize(sz) if up != sz { nbuckets = up / t.bucket.size //得出桶的数量 } } if dirtyalloc == nil { // 没有被创建桶，申请创建桶的，返回桶的首地址 buckets = newarray(t.bucket, int(nbuckets)) } else { // dirtyalloc was previously generated by // the above newarray(t.bucket, int(nbuckets)) // but may not be empty. buckets = dirtyalloc size := t.bucket.size * nbuckets if t.bucket.ptrdata != 0 { memclrHasPointers(buckets, size) } else { memclrNoHeapPointers(buckets, size) } } if base != nbuckets { // We preallocated some overflow buckets. // To keep the overhead of tracking these overflow buckets to a minimum, // we use the convention that if a preallocated overflow bucket's overflow // pointer is nil, then there are more available by bumping the pointer. // We need a safe non-nil pointer for the last overflow bucket; just use buckets. //空闲桶的地址 nextOverflow = (*bmap)(add(buckets, base*uintptr(t.bucketsize))) last := (*bmap)(add(buckets, (nbuckets-1)*uintptr(t.bucketsize))) last.setoverflow(t, (*bmap)(buckets)) } return buckets, nextOverflow }  如图所示\n\r 使用map时需要make(map[type]type,len,cap)才能使用。 make 源码如下：    func makemap(t *maptype, hint int, h *hmap) *hmap { // 判断是否超过内存的限制 mem, overflow := math.MulUintptr(uintptr(hint), t.bucket.size) if overflow || mem \u0026gt; maxAlloc { hint = 0 } // initialize Hmap if h == nil { h = new(hmap) } h.hash0 = fastrand()// 获取随机的hash值 // Find the size parameter B which will hold the requested # of elements. // For hint \u0026lt; 0 overLoadFactor returns false since hint \u0026lt; bucketCnt. B := uint8(0) for overLoadFactor(hint, B) { B++ } h.B = B // allocate initial hash table // if B == 0, the buckets field is allocated lazily later (in mapassign) // If hint is large zeroing this memory could take a while. if h.B != 0 { var nextOverflow *bmap // 创建map的存储数据，返回的桶的数据的地址，下一个溢出桶的地址 h.buckets, nextOverflow = makeBucketArray(t, h.B, nil) if nextOverflow != nil { h.extra = new(mapextra) h.extra.nextOverflow = nextOverflow } } return h }   map的完整结构如图： \r\nmap扩容 扩容条件     当负载因子(loadFactorNum*(bucketShift(B)/loadFactorDen\u0026gt;6.5 -\u0026gt; 翻倍扩容\n  当负载因子小于6.5，但是溢出桶的数量大于2的15次方 -\u0026gt; 等量扩容\n  源代码如下：\n    // overLoadFactor reports whether count items placed in 1\u0026lt;\u0026lt;B buckets is over loadFactor. // 负载因子大于6.5 func overLoadFactor(count int, B uint8) bool { return count \u0026gt; bucketCnt \u0026amp;\u0026amp; uintptr(count) \u0026gt; loadFactorNum*(bucketShift(B)/loadFactorDen) } // 溢出桶过多时 func tooManyOverflowBuckets(noverflow uint16, B uint8) bool { // If the threshold is too low, we do extraneous work. // If the threshold is too high, maps that grow and shrink can hold on to lots of unused memory. // \u0026quot;too many\u0026quot; means (approximately) as many overflow buckets as regular buckets. // See incrnoverflow for more details. if B \u0026gt; 15 { B = 15 } // The compiler doesn't see here that B \u0026lt; 16; mask B to generate shorter shift code. return noverflow \u0026gt;= uint16(1)\u0026lt;\u0026lt;(B\u0026amp;15) } // 扩容源码 func hashGrow(t *maptype, h *hmap) { // If we've hit the load factor, get bigger. // Otherwise, there are too many overflow buckets, // so keep the same number of buckets and \u0026quot;grow\u0026quot; laterally. bigger := uint8(1) if !overLoadFactor(h.count+1, h.B) { //等量扩容 bigger = 0 h.flags |= sameSizeGrow } oldbuckets := h.buckets newbuckets, nextOverflow := makeBucketArray(t, h.B+bigger, nil)// 从新分配数据地址 flags := h.flags \u0026amp;^ (iterator | oldIterator) if h.flags\u0026amp;iterator != 0 { // 迭代的时候搬迁旧桶 flags |= oldIterator } // commit the grow (atomic wrt gc) h.B += bigger // 桶的个数 h.flags = flags h.oldbuckets = oldbuckets h.buckets = newbuckets h.nevacuate = 0 h.noverflow = 0 // 溢出桶钻便为旧溢出桶 if h.extra != nil \u0026amp;\u0026amp; h.extra.overflow != nil { // Promote current overflow buckets to the old generation. if h.extra.oldoverflow != nil { throw(\u0026quot;oldoverflow is not nil\u0026quot;) } h.extra.oldoverflow = h.extra.overflow h.extra.overflow = nil } if nextOverflow != nil { if h.extra == nil { h.extra = new(mapextra) } h.extra.nextOverflow = nextOverflow } // the actual copying of the hash table data is done incrementally // by growWork() and evacuate(). } 参考文献 1.https://www.zhihu.com/people/kylin-lab\n","date":"2021-09-20T22:00:38+08:00","image":"https://zcj-git520.github.io/p/go-map%E7%9A%84%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3/3_hu50c7ad8b87ebb1655f26b2166388bb8a_150234_120x120_fill_box_smart1_3.png","permalink":"https://zcj-git520.github.io/p/go-map%E7%9A%84%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3/","title":"go map的深入理解"},{"content":"服务器配置(服务器平台：x86) Rsyslog简介  Rsyslog是一个 syslogd 的多线程增强版，在syslog的基础上扩展了很多其他功能，如数据库支持(MySQL, PostgreSQL、Oracle等)、日志内容筛选、定义日志格式模板等。除了默认的udp协议外，rsyslog还支持tcp协议来接收日志。 目前的linux的发行版都切换为rsyslog  安装Rsyslog  Linux的发行版中预先安装了Rsyslog,无需安装，rsyslogd –v 查看版本 若未安装，以下是安装步骤： 1.ubuntu：sudo apt install rsyslog 2.CentOS：yum install rsyslog  Rsyslog.conf配置文件详解 配置文件位置：/etc/rsyslog.conf #### MODULES #### #定义日志的模块。 $ModLoad imuxsock #imuxsock为模块名，支持本地系统日志的模块。 $ModLoad imjournal #imjournal为模块名，支持对系统日志的访问。 #$ModLoad imklog #imklog为模块名，支持内核日志的模块。 #$ModLoad immark #immark为模块名，支持日志标记。 # Provides UDP syslog reception #提供udp syslog的接收。 #$ModLoad imudp #imudp为模块名，支持udp协议。 #$UDPServerRun 514 #允许514端口接收使用udp和tcp转发来的日志。 # Provides TCP syslog reception #提供tcp syslog的接收。 #$ModLoad imtcp #imtcp为模块名，支持tcp协议。 #$InputTCPServerRun 514 #### GLOBAL DIRECTIVES #### #定义全局日志格式的指令。 # Where to place auxiliary files $WorkDirectory /var/lib/rsyslog #工作目录。 # Use default timestamp format $ActionFileDefaultTemplate RSYSLOG_TraditionalFileFormat #定义日志格式默认模板。 $IncludeConfig /etc/rsyslog.d/*.conf #所有配置文件路径。 $OmitLocalLogging on #省略本地登录。 # File to store the position in the journal $IMJournalStateFile imjournal.state #### RULES #### #kern.* /dev/console #记录所有日志类型的info级别以及大于info级别的信息到messages文件，但是mail邮件信息，authpriv验证方面的信息和corn时间和任务相关信息除外。 *.info;mail.none;authpriv.none;cron.none /var/log/messages # authpriv验证相关的所有信息存放在/var/log/secure。 authpriv.* /var/log/secure #邮件的所有信息存在/var/log/maillog；这里有一个“-”符号表示是使用异步的方式记录 mail.* -/var/log/maillog #任务计划有关的信息存放在/var/log/cron。 cron.* /var/log/cron #记录所有的≥emerg级别信息，发送给每个登录到系统的日志。 *.emerg :omusrmsg:* #记录uucp，news.crit等存放在/var/log/spooler uucp,news.crit /var/log/spooler #本地服务器的启动的所有日志存放在/var/log/boot.log local7.* /var/log/boot.log 以下为：rsyslog 客服端的配置 #发送日志，@表示传输协议（@表示udp，@@表示tcp），后面是ip和端口。 #*.* @@remote-host:514 配置服务器  使用sudo vi /etc/rsyslog.conf 打开配置文件  选择传输的协议 1.使用udp传输日志，配置时将前面的#去掉即可\n$ModLoad imudp\n$UDPServerRun 514\n2.使用tcp传输协议, 将#去点即可\n$ModLoad imtcp\n$InputTCPServerRun 514\n配置如图：\r\n 注明：\n514/5014端口号可以自己配置，默认为514.\nrsyslog后台进程是可以同时监听TCP/UDP连接的  配置接收日志模板  在GLOBAL DIRECTIVES内容块的前面增加接收日志模板 模板如下：\n$template RemoteLogs,\u0026quot;/var/log/%HOSTNAME%/%PROGRAMNAME%.log\u0026quot;\n*.* ?RemoteLogs\n\u0026amp; ~ 注明：\n1、$template RemoteLogs指令（“RemoteLogs” 可以为其它的描述的名字）迫使rsyslog后台进程隔开本地/var/log/下文件去写日志信息。而日志文件名则依据发送远程日志的机器名及应用程序名来定义。\n2、*.* ?RemoteLogs）暗含运行用模板RemoteLogs于所有的接收日志。\n3、\u0026amp; ~则告诉rsyslog后台进程停止进一步的去处理日志信息,即不对它们进行本地化写入，它是代表一个重定向规则。如果没有这一行，则意味着接收到的日志会写入两次，一次如前两行写的方式写，第二次则以本地日志记录的方式写入。运行这个规则的另一个结论则是日志服务器自己的日志信息只会写入到依照机器主机名命名的文件中。 \r 设置后，会按照模板格式保存日志 \r  设置完成，保存 检查Rsyslog配置  使用命令：rsyslogd -f /etc/rsyslog.conf -N1 配置信息正确有如下提示: \r 若配置信息有误，则需要在更改配置文件  重启RSyslog服务  Debian,Ubuntu或CentOS/RHEL 6使用:sudo service rsyslog restart Fedora 或 CentOS/RHEL 7使用：sudo systemctl restart rsyslog 使用：sudo lsof -i :[端口号]，查看服务是否开启和tcp/udp连接情况 \r 或使用sudo netstat -pantu | grep rsyslog \r  查看日志  启动客户端，进入配置文件模板中日志保存的位置：\n\r\n\r 若没有生成日志文件，需要使用：sudo tcpdump host 客户端ip 查看是否转发日志，有则是保存模板出问题，没有可能的服务端配置或者客户端配置出问题\n\r  客户端配置 平台：x86  平台是使用的rsyslog 使用sudo vi /etc/rsyslog.conf 打开配置文件 配置：\n发送日志，@表示传输协议（@表示udp，@@表示tcp），后面是ip和端口。\n*.* @@remote-host:514  \r\n 检查与重启服务和x86服务器平台一样  平台开发板  板子是使用的syslog 使用命令：ps进行查看进程\n\r 找到syslogd服务，若未找到，可能不支持syslog服务 使用命令：syslogd –h 查看syslogd支持的服务\n\r  配置客户端 1.结束之前的syslogd服务，通过使用kill 进程号 结束进程\n\r 2.通过syslogd提供的服务是通过 -L –R 服务器的ip:port\n注明：默认端口为：514,端口号应该与服务器配置的端口号一致\n3.启动syslogd服务：/sbin/syslogd -f /etc/syslog.conf -L -R 172.16.193.204:514 4.通过tcpdump host 172.16.193.204 查看日志是否被转发到服务器\n\r\n 注明：开发板的syslog支持UDP传输，需要开发板的日志文件需要在服务器打开UDP  ","date":"2021-09-20T22:00:38+08:00","image":"https://zcj-git520.github.io/p/syslog%E6%97%A5%E5%BF%97%E8%BD%AC%E5%8F%91%E9%85%8D%E7%BD%AE/8_hu442c80cfdca83249146ebd0d5822289f_88346_120x120_fill_box_smart1_3.png","permalink":"https://zcj-git520.github.io/p/syslog%E6%97%A5%E5%BF%97%E8%BD%AC%E5%8F%91%E9%85%8D%E7%BD%AE/","title":"syslog日志转发配置"},{"content":"切片的内部结构  切片的结构可分为：数组，数据（元素）的地址\u0026amp;data、也存元素个数len、可以存储多少元素cap 源码如下    定义切片的结构： type slice struct { array unsafe.Pointer len int cap int }  如图所示  \ravatar\r\n var data [] int 声明一个切片，相当于生成切片的结构，data地址指针为nil, len和cap都为0。这就很清楚为什么，nil切片不可以直接使用了😄 结构如图  \ravatar\r\n 使用切片时需要make([]type,len,cap)或者初始化[]type{}才能使用，这是因为在在生成切片的结构时，同时也开辟了一段新的内存，类型为type, 结构长度为cap,同时值进行初始化。 make 源码如下：    func makeslice(et *_type, len, cap int) unsafe.Pointer { mem, overflow := math.MulUintptr(et.size, uintptr(cap)) // 判断是否越界 if overflow || mem \u0026gt; maxAlloc || len \u0026lt; 0 || len \u0026gt; cap { // NOTE: Produce a 'len out of range' error instead of a // 'cap out of range' error when someone does make([]T, bignumber). // 'cap out of range' is true too, but since the cap is only being // supplied implicitly, saying len is clearer. // See golang.org/issue/4085. mem, overflow := math.MulUintptr(et.size, uintptr(len)) if overflow || mem \u0026gt; maxAlloc || len \u0026lt; 0 { panicmakeslicelen() // 越界直接 panic } panicmakeslicecap() // 越界直接 panic } return mallocgc(mem, et, true) //开辟内存 }  \ravatar\r\n   也可以通过底层数组初始化，切片的data指针指向就是相同类型的底层数组；通过slince := array[n:m],表示定义了一个类型和array相同，len为m-n,cap默认为array的长度的切片。切片和数组都指向了相同的地址。多个切片可以共用同一个底层数组。 \ravatar\r\n  通过append 函数向切片增加切片的元素，增加了len, cap 不变。\n切片扩容   在资源充裕的条件下，切片是可以通过append不断增加元素，当len个数增加到cap一样时，在增加元素时，就需要增加切片的容量cap，那问题来了，切片是怎么扩容的呢？\n扩容规则（预估规则）     当需要扩容的数量比之前cap的两倍都大，则扩容为需要扩容的数量\n  当需要扩容的数量比之前cap的两倍都大小，之前的cap小于1024 直接扩大之前的2倍\n  当需要扩容的数量比之前cap的两倍都大小，之前的cap大于1024 直接扩大之前的1.25倍\n  伪代码如下\n if oldcap2 \u0026lt; newcap 时， 扩容为newcap else{ if oldcap \u0026lt; 1024 newcap = 2oldcap ; else newcap = 1.25*oldcap }\n   源代码如下：\n    newcap := old.cap doublecap := newcap + newcap //两倍的oldcap if cap \u0026gt; doublecap { //当需要扩容的数量比之前cap的两倍都大，则扩容为需要扩容的数量 newcap = cap } else { //当需要扩容的数量比之前cap的两倍都大小，之前的cap小于1024 直接扩大之前的2倍 if old.cap \u0026lt; 1024 { newcap = doublecap } else { // Check 0 \u0026lt; newcap to detect overflow // and prevent an infinite loop. 当需要扩容的数量比之前cap的两倍都大小，之前的cap大于1024 直接扩大之前的1.25倍 for 0 \u0026lt; newcap \u0026amp;\u0026amp; newcap \u0026lt; cap { newcap += newcap / 4 } // Set newcap to the requested cap when // the newcap calculation overflowed. if newcap \u0026lt;= 0 { newcap = cap } } } 扩容调整  在预估扩容后，会根据内存对齐（减少内存浪费）在进行调整，代码：capmem := roundupsize(uintptr(newcap) * uintptr(et.size))newcap就是前文中计算出的newcap，et.size代表slice中一个元素的大小，capmem计算出来的就是此次扩容需要申请的内存大小。roundupsize函数就是处理内存对齐的函数 源码如下   var overflow bool var lenmem, newlenmem, capmem uintptr switch { case et.size == 1: //例如byte 大小为1， 扩容的大小为向上取整的数值 lenmem = uintptr(old.len) newlenmem = uintptr(cap) capmem = roundupsize(uintptr(newcap)) overflow = uintptr(newcap) \u0026gt; maxAlloc newcap = int(capmem) case et.size == sys.PtrSize: lenmem = uintptr(old.len) * sys.PtrSize newlenmem = uintptr(cap) * sys.PtrSize capmem = roundupsize(uintptr(newcap) * sys.PtrSize) overflow = uintptr(newcap) \u0026gt; maxAlloc/sys.PtrSize newcap = int(capmem / sys.PtrSize) case isPowerOfTwo(et.size): //处理2的倍数 var shift uintptr if sys.PtrSize == 8 { // Mask shift for better code generation. shift = uintptr(sys.Ctz64(uint64(et.size))) \u0026amp; 63 } else { shift = uintptr(sys.Ctz32(uint32(et.size))) \u0026amp; 31 } lenmem = uintptr(old.len) \u0026lt;\u0026lt; shift newlenmem = uintptr(cap) \u0026lt;\u0026lt; shift capmem = roundupsize(uintptr(newcap) \u0026lt;\u0026lt; shift) overflow = uintptr(newcap) \u0026gt; (maxAlloc \u0026gt;\u0026gt; shift) newcap = int(capmem \u0026gt;\u0026gt; shift) default: lenmem = uintptr(old.len) * et.size newlenmem = uintptr(cap) * et.size capmem, overflow = math.MulUintptr(et.size, uintptr(newcap)) capmem = roundupsize(capmem) newcap = int(capmem / et.size) } // The check of overflow in addition to capmem \u0026gt; maxAlloc is needed // to prevent an overflow which can be used to trigger a segfault // on 32bit architectures with this example program: // // type T [1\u0026lt;\u0026lt;27 + 1]int64 // // var d T // var s []T // // func main() { // s = append(s, d, d, d, d) // print(len(s), \u0026quot;\\n\u0026quot;) // } if overflow || capmem \u0026gt; maxAlloc { panic(errorString(\u0026quot;growslice: cap out of range\u0026quot;)) } ### 扩容后内存分配 * 分配 大于cap的内存，没有数据指针，memclrNoHeapPointers创建 * 源码如下： \u0026gt; var p unsafe.Pointer if et.ptrdata == 0 { p = mallocgc(capmem, nil, false) // The append() that calls growslice is going to overwrite from old.len to cap (which will be the new length). // Only clear the part that will not be overwritten. memclrNoHeapPointers(add(p, newlenmem), capmem-newlenmem) } else { // Note: can't use rawmem (which avoids zeroing of memory), because then GC can scan uninitialized memory. p = mallocgc(capmem, et, true) //分配内存地址 if lenmem \u0026gt; 0 \u0026amp;\u0026amp; writeBarrier.enabled { // Only shade the pointers in old.array since we know the destination slice p // only contains nil pointers because it has been cleared during alloc. bulkBarrierPreWriteSrcOnly(uintptr(p), uintptr(old.array), lenmem-et.size+et.ptrdata) } } memmove(p, old.array, lenmem) //数据迁移 return slice{p, old.len, newcap} } ","date":"2021-09-15T22:00:38+08:00","image":"https://zcj-git520.github.io/p/go-%E5%88%87%E7%89%87%E7%9A%84%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3/s4_hu1fce009c184102799b4a69d72efc8ec2_68940_120x120_fill_box_smart1_3.png","permalink":"https://zcj-git520.github.io/p/go-%E5%88%87%E7%89%87%E7%9A%84%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3/","title":"go 切片的深入理解"},{"content":"为什么写博客  总结开发中遇到的问题。工作过后发现自己并不擅长对知识点的总结，导致总是遇到相同的问题，过段时间需要重新查找解决方案 记录学习的知识，不断的温习。学的东西过于碎片化，导致知识不成体系。时间长了，碎片的知识也忘记了 提升自己的专业技能。通过写博客提升自己的能力 形成自己的技术栈，遇到的志同道合的朋友  为什么选择hugo来搭建自己的博客  Hugo是由Go语言实现的静态网站生成器。简单、易用、高效、易扩展、快速部署。 操作简单，使用Markdown直接生成静态网页 免费且以维护, 在github上就可供他人访问，无需购买服务器，维护简单 发表文章直接push到自己仓库即可  下载hogo的源码  git clone https://github.com/gohugoio/hugo.git\ngit branch 查看单前代码的分支\ngit branch -a 查看全部分支\ngit checkout branch 切换分支\ngit branch 分支名 创建自己的本地分子\n 编译源码  在master分支下，在main.go 的目录下使用命令: go build 在目录下生成hugo.exe 在cmd下使用hugo 查看是否编译成功 编译成功 会打印hugo的版本 安装成功  生成站点  使用命令：hugo new site /目录 cd /目录 查看到   ▸ archetypes/ ▸ content/ ▸ layouts/ ▸ static/ config.toml   创建站点成功  创建md文章  使用命令: hugo new 文章名.md 在content/ 下生成该md文件  选择博客主题模板  hugo 提供很多的主题博客模板：https://themes.gohugo.io/ 创建theme文件夹，将主题模板放在里面 ：mkdir themes 进入该文件夹：cd themes 下载主题，使用git clone 主题模板 ：git clone https://github.com/spf13/hyde.git  配置config.toml文件  config.toul 文件hugo 的配置文件，可以配置主题模板，个人信息等(主题模板中相应的配置文件)如   baseurl = \u0026quot;http://****.com/\u0026quot; //发布的网站 languageCode = \u0026quot;ja\u0026quot; //使用的语言 title = \u0026quot;xxxx.COM\u0026quot; //网站名称等 [Params] subtitle = \u0026quot;I would like to be a layer 3 switch.\u0026quot; facebook = \u0026quot;https://facebook.com/foobar\u0026quot; twitter = \u0026quot;https://twitter.com/foobar\u0026quot; github = \u0026quot;https://github.com/foobar\u0026quot; profile = \u0026quot;/images/profile.png\u0026quot; copyright = \u0026quot;Written by Asuka Suzuki\u0026quot; analytics = \u0026quot;UA-XXXXXXXX-X\u0026quot;    运行 本地运行  使用命令：hugo server \u0026ndash;buildDrafts 配置正确则会出现： http://localhost:1313/ (bind address 127.0.0.1) 点击在浏览器中运行  推送到gitgub  首先在GitHub上创建一个Repository，命名为：github用户名.github.io 修改config.toml 配置文件：将baseurl = \u0026ldquo;http://github用户名.github.io\u0026rdquo; 使用命令：hugo \u0026ndash;buildDrafts 在本地生成public的文件夹 \u0026ndash;buildDrafts 参数的主用是将你的文章在主题中出现   cd public 进入到public文件夹 $ git init 初始化本地仓库 $ git remote add origin https://github.com/github用户名/github用户名.github.io //添加原创仓库 或者直接 git clone $ git add -A $ git commit -m \u0026quot;first commit\u0026quot; $ git push -u origin master //推到远端   使用 \u0026ldquo;http://github用户名.github.io\u0026quot;就可访问  ","date":"2021-09-04T10:05:40+08:00","permalink":"https://zcj-git520.github.io/p/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E4%BB%BD%E5%8D%9A%E5%AE%A2/","title":"我的第一份博客"}]