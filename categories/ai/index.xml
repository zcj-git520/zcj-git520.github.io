<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI on Chengji Zhao&#39;s blog</title>
    <link>https://zcj-git520.github.io/categories/ai/</link>
    <description>Recent content in AI on Chengji Zhao&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Mon, 30 Jun 2025 22:00:38 +0800</lastBuildDate><atom:link href="https://zcj-git520.github.io/categories/ai/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>深入探索 LangGraph</title>
      <link>https://zcj-git520.github.io/p/%E6%B7%B1%E5%85%A5%E6%8E%A2%E7%B4%A2-langgraph/</link>
      <pubDate>Mon, 30 Jun 2025 22:00:38 +0800</pubDate>
      
      <guid>https://zcj-git520.github.io/p/%E6%B7%B1%E5%85%A5%E6%8E%A2%E7%B4%A2-langgraph/</guid>
      <description>深入探索 LangGraph 在人工智能快速发展的今天，构建能够自主决策、灵活调用工具的智能代理成为了许多开发者的目标。LangGraph 作为一个强大的框架，为我们提供了构建这类代理的理想工具。本文将深入探讨 LangGraph 的核心概念、架构设计以及实际应用，帮助你快速掌握这一框架的使用。
什么是 LangGraph？ LangGraph 是一个基于图论的框架，用于构建具有复杂决策能力的智能代理。它允许开发者将代理的行为建模为一个状态图，其中包含节点（代表计算步骤）和边（代表节点间的连接）。这种结构使得代理能够根据当前状态做出决策，选择合适的工具，并在必要时调整其行为。
与传统的线性工作流不同，LangGraph 提供的图结构赋予了代理更强的灵活性和自主性。代理可以根据输入动态选择执行路径，多次调用不同工具，并在获得新信息时调整其策略。
LangGraph 的核心概念 状态（State） 状态是 LangGraph 中最基础的概念，它表示应用程序在某一时刻的快照。状态通常定义为一个 TypedDict 或 Pydantic 模型，包含代理在执行过程中需要跟踪的所有信息。
class State(TypedDict):messages: Annotated[list, add_messages]在这个例子中，我们定义了一个简单的状态，只包含一个 messages 字段，用于存储对话历史。Annotated[list, add_messages] 表示这个字段将使用 add_messages 归约器，这意味着新消息会被添加到列表中，而不是替换整个列表。
节点（Nodes） 节点是图中的基本单位，代表一个计算步骤或状态转换。每个节点都是一个函数，接收当前状态作为输入，执行某些操作，并返回更新后的状态。
def node(state: State):messages = state[&amp;quot;messages&amp;quot;]response = model.qwen_llm().invoke(messages)return {&amp;quot;messages&amp;quot;: [response]}这个示例展示了一个简单的节点，它接收当前状态中的消息，调用语言模型生成响应，并将响应添加到消息列表中。
LangGraph 支持多种类型的节点：
  计算节点：执行计算或转换操作
  条件节点：根据状态条件执行不同路径
  工具节点：调用外部工具（如 API、数据库等）
  边（Edges） 边定义了图中节点之间的连接关系，控制着程序的执行流程。LangGraph 提供了多种类型的边：
 普通边：直接从一个节点连接到另一个节点  graph_builder.add_edge(&amp;quot;node_a&amp;quot;, &amp;quot;node_b&amp;quot;) 条件边：根据状态动态决定下一个节点  graph_builder.</description>
    </item>
    
    <item>
      <title>LangGraph预构建Agent</title>
      <link>https://zcj-git520.github.io/p/langgraph%E9%A2%84%E6%9E%84%E5%BB%BAagent/</link>
      <pubDate>Wed, 25 Jun 2025 22:00:38 +0800</pubDate>
      
      <guid>https://zcj-git520.github.io/p/langgraph%E9%A2%84%E6%9E%84%E5%BB%BAagent/</guid>
      <description>LangGraph 预构建 Agent 智能代理（Agent）系统已成为连接用户与复杂功能的重要桥梁。LangGraph 作为一个强大的框架，为开发者提供了构建健壮、可投入生产的代理系统的丰富工具。
一、Agent 核心组件解析 LangGraph 预构建的 Agent 由三个核心部分构成，它们协同工作，使 Agent 能够理解用户需求、执行任务并返回结果：
1.大型语言模型（LLM）
这是 Agent 的 &amp;ldquo;大脑&amp;rdquo;，负责理解文本、生成响应并决定下一步行动。LLM 在一个循环中运行：每次迭代中选择工具、提供输入、接收结果（观察），并根据观察指导下一个动作，直到收集到足够信息响应用户。
2.工具（Tools）
工具是 Agent 可以调用的函数或 API，用于执行特定任务。例如：
  网络搜索工具
  数据库查询工具
  第三方服务调用工具（如天气查询、IP 解析等）
  工具使 Agent 能够突破自身知识局限，获取实时信息或执行复杂操作。
3.提示（Prompt）
提示是指导 LLM 行为的文本指令。它定义了 Agent 的角色、目标和行为准则。例如：&amp;ldquo;你是一个智能小助手，能够使用工具帮助用户查询天气和城市信息&amp;rdquo;。
二、LangGraph 的核心优势 LangGraph 为构建 Agent 系统提供了多项关键功能，使其在实际生产环境中表现出色：
  内存集成：同时支持短期（会话内）和长期（跨会话）内存，实现有状态行为，让 Agent 能够记住历史对话和用户偏好。
  人在回路控制：允许执行过程在任何点暂停，等待人工反馈或审批，支持异步干预，这对于需要人工确认的敏感操作至关重要。
  流式传输支持：能够实时流式传输 Agent 状态、模型输出和工具结果，提升用户体验。
  部署工具：提供完整的测试、调试和部署工具链，简化从开发到生产的流程。
  三、创建和配置 Agent 使用 LangGraph 创建预构建 Agent 非常简单，以下是基本步骤：</description>
    </item>
    
    <item>
      <title>智能知识检索系统</title>
      <link>https://zcj-git520.github.io/p/%E6%99%BA%E8%83%BD%E7%9F%A5%E8%AF%86%E6%A3%80%E7%B4%A2%E7%B3%BB%E7%BB%9F/</link>
      <pubDate>Mon, 02 Jun 2025 22:00:38 +0800</pubDate>
      
      <guid>https://zcj-git520.github.io/p/%E6%99%BA%E8%83%BD%E7%9F%A5%E8%AF%86%E6%A3%80%E7%B4%A2%E7%B3%BB%E7%BB%9F/</guid>
      <description>构建智能知识检索系统：基于 Streamlit 与 LangChain 的实现方案 在信息爆炸的时代，如何高效管理和检索文档中的知识成为一项重要需求。本文将介绍一个基于 Streamlit 和 LangChain 构建的智能知识检索系统，该系统支持多种格式文档的上传、解析与检索，能够通过自然语言交互为用户提供精准的知识问答服务。
系统架构概览 该智能知识检索系统主要由以下几个核心模块构成：
 用户界面层：基于 Streamlit 构建的 Web 交互界面 文档处理层：负责解析多种格式的文档内容 向量数据库层：使用 ChromaDB 存储文档向量，支持高效检索 智能问答层：基于 LangChain 的 Agent 机制实现问答逻辑  系统的核心流程是：用户上传文档 → 系统解析并存储文档向量 → 用户提问 → 系统检索相关文档 → 生成回答。

核心代码解析 类结构设计 系统的核心功能封装在KnowledgeBaseSystem类中，该类通过组合模式整合了模型、数据库和文档处理器：
class KnowledgeBaseSystem: def __init__(self, model, chromadb: ChromaDB, document_processor: Knowledge): self.chromadb = chromadb self.model = model self.document_processor = document_processor 页面配置与初始化 setup_page_config方法负责初始化 Streamlit 页面配置和会话状态：
def setup_page_config(self): &amp;#34;&amp;#34;&amp;#34;设置页面配置&amp;#34;&amp;#34;&amp;#34; st.set_page_config( page_title=&amp;#34;智能知识检索系统&amp;#34;, layout=&amp;#34;wide&amp;#34;, page_icon=&amp;#34;📚&amp;#34; ) st.</description>
    </item>
    
    <item>
      <title>检索增强生成（RAG）</title>
      <link>https://zcj-git520.github.io/p/%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90rag/</link>
      <pubDate>Sun, 25 May 2025 22:00:38 +0800</pubDate>
      
      <guid>https://zcj-git520.github.io/p/%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90rag/</guid>
      <description>检索增强生成（RAG）：从原理到工程实践的深度解析 在大语言模型（LLM）快速迭代的今天，如何让模型既具备通用智能，又能精准调用特定领域知识，成为企业级 AI 应用的核心挑战。检索增强生成（Retrieval-Augmented Generation，RAG）技术通过 &amp;ldquo;检索 + 生成&amp;rdquo; 的协同模式，成功解决了纯 LLM 在知识时效性、准确性和领域适配性上的短板，成为连接通用 AI 与垂直业务的关键桥梁。本文将从技术原理出发，结合实战代码，全面剖析 RAG 的实现路径与工程实践。
一、RAG 的基本概念和原理 RAG 技术的核心思想源于信息检索与自然语言生成的融合：在生成回答前，先从外部知识库中检索与问题相关的事实性信息，再让模型基于这些 &amp;ldquo;证据&amp;rdquo; 进行推理生成。这种机制将 LLM 的 &amp;ldquo;闭卷考试&amp;rdquo; 模式转变为 &amp;ldquo;开卷考试&amp;rdquo;，从根本上解决了三个核心问题：
 知识时效性局限：LLM 训练数据存在截止日期，无法应对动态更新的业务知识（如 2025 年的新法规、企业内部文档更新）； 幻觉生成风险：模型可能编造不存在的事实（如虚构产品参数、学术引用），在医疗、法律等领域可能引发严重后果； 领域知识壁垒：通用模型对专业领域（如网络安全、金融风控）的深度知识掌握不足，难以生成精准回答。  从技术本质看，RAG 是一种混合增强架构：通过检索模块将外部知识引入生成过程，形成 &amp;ldquo;知识输入 - 模型推理 - 结果输出&amp;rdquo; 的闭环。这种架构既保留了 LLM 的上下文理解与自然语言生成能力，又通过外部知识库实现了知识的可控更新与精准调用。
流程图
可以加密文本块和向量（向量也可能泄露原始语义），从而使用密文进行存储与网络传输。对于向量加密需采用特殊加密算法使得加密后向量仍能进行相似度检索。 加密流程图
二、RAG 的工作流程 一个完整的 RAG 系统可分为离线知识库构建与在线问答交互两大阶段，包含五个核心步骤：
1. 离线知识库构建  文档采集：收集结构化（如 Excel 表格）、半结构化（如 PDF 手册）和非结构化（如图片、音频转文本）数据； 文档预处理：对原始数据进行清洗（去噪、格式统一）、拆分（按语义单元切割长文档）； 向量编码：通过嵌入模型（Embedding Model）将文本片段转化为高维向量，捕捉语义特征； 向量存储：将向量及关联文本存入向量数据库，建立可检索的知识索引。</description>
    </item>
    
    <item>
      <title>LangChain 自定义工具</title>
      <link>https://zcj-git520.github.io/p/langchain-%E8%87%AA%E5%AE%9A%E4%B9%89%E5%B7%A5%E5%85%B7/</link>
      <pubDate>Tue, 25 Mar 2025 22:00:38 +0800</pubDate>
      
      <guid>https://zcj-git520.github.io/p/langchain-%E8%87%AA%E5%AE%9A%E4%B9%89%E5%B7%A5%E5%85%B7/</guid>
      <description>LangChain 自定义工具 引言 在构建 LangChain 代理时，工具(Tools)是核心组件之一。工具允许代理与外部世界交互，执行特定任务。本文将深入探讨如何在 LangChain 中创建和使用自定义工具，包括内置工具的使用和自定义工具的多种实现方式。
工具的基本组成 每个工具都包含几个关键组件：
 name (str): 必需且必须在工具集中唯一 description (str): 可选但建议提供，代理用它来决定如何使用工具 return_direct (bool): 默认为 False args_schema (Pydantic BaseModel): 可选但建议提供，可用于验证参数或提供few-shot示例  内置工具的使用 LangChain 提供了许多内置工具，请访问工具集成查看可用工具列表，在使用第三方工具时，请确保您了解工具的工作原理、权限情况：
维基百科查询工具 def get_wikipedia(query): api_wrapper = WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=100) tool = WikipediaQueryRun(api_wrapper=api_wrapper) result = tool.run(query) return result Google 文字转语音工具 def google_tts(text): tts = GoogleCloudTextToSpeechTool() speech_file = tts.run(text) return speech_file 自定义工具的实现方式 1. 使用 @tool 装饰器 这是定义自定义工具最简单的方式：
from langchain_core.tools import tool @tool def search_city_tool(city: str) -&amp;gt; str: &amp;#34;&amp;#34;&amp;#34;搜索城市信息&amp;#34;&amp;#34;&amp;#34; url = f&amp;#34;https://api.</description>
    </item>
    
    <item>
      <title>LangChain 多模态输入处理</title>
      <link>https://zcj-git520.github.io/p/langchain-%E5%A4%9A%E6%A8%A1%E6%80%81%E8%BE%93%E5%85%A5%E5%A4%84%E7%90%86/</link>
      <pubDate>Tue, 25 Feb 2025 22:00:38 +0800</pubDate>
      
      <guid>https://zcj-git520.github.io/p/langchain-%E5%A4%9A%E6%A8%A1%E6%80%81%E8%BE%93%E5%85%A5%E5%A4%84%E7%90%86/</guid>
      <description>LangChain 多模态输入处理 引言 随着大模型技术的发展，多模态能力已成为AI应用的重要方向。LangChain提供了强大的多模态处理能力，允许开发者将文本、图像、PDF等多种输入类型结合使用。本文将深入探讨LangChain中的多模态输入处理机制。
多模态基础概念 多模态(Multimodality)是指模型能够同时处理和生成多种类型的数据，包括但不限于：
 文本(Text) 图像(Image) 音频(Audio) 视频(Video) 文档(Document)  LangChain多模态架构 LangChain的多模态处理主要基于以下组件：
 HumanMessage: 承载多模态内容的核心消息类 内容类型标识(type字段): 标识不同媒体类型 数据来源标识(source_type字段): 区分本地base64或URL  多模态输入实现方式 1. 基于Base64的本地文件输入 def input_by_base64(self, prompt_txt, image_data, type): try: message = HumanMessage( content=json.dumps([ {&amp;#34;type&amp;#34;: &amp;#34;text&amp;#34;, &amp;#34;text&amp;#34;: prompt_txt}, {&amp;#34;type&amp;#34;: type, &amp;#34;data&amp;#34;: image_data, &amp;#34;source_type&amp;#34;: &amp;#34;base64&amp;#34;}, ]) ) china = self.model.qwen_llm() return china.invoke([message]).content except Exception as e: print(e) return &amp;#34;&amp;#34; 使用场景：
 处理本地存储的图片、PDF等文件 需要数据隐私保护的场景 离线应用环境  2. 基于URL的网络资源输入 def input_by_url(self, prompt_txt, url, type): try: message = HumanMessage( content=json.</description>
    </item>
    
    <item>
      <title>LangServe</title>
      <link>https://zcj-git520.github.io/p/langserve/</link>
      <pubDate>Sat, 25 Jan 2025 22:00:38 +0800</pubDate>
      
      <guid>https://zcj-git520.github.io/p/langserve/</guid>
      <description>LangServe 实践 核心功能 LangServe 是一个专为 LangChain 设计的部署工具，可将 LangChain 可运行对象和链快速转化为 REST API，主要特点包括：
 自动化 API 生成
通过 FastAPI + Pydantic 自动推断输入/输出模型，生成带数据验证的 REST 端点 多协议支持
提供 /invoke（单次调用）、/batch（批量处理）、/stream（实时流）等标准化端点 调试友好  /stream_log 实时流式传输中间步骤 0.0.40+ 版本新增 /stream_events 简化流式处理   生产级架构
基于 uvloop 和 asyncio 实现高并发，支持 Swagger 自动文档  技术优势    特性 说明     声明式验证 通过 Pydantic 模型自动生成 JSON Schema，提供类型检查和清晰错误提示   无缝集成 内置 LangChain.js 客户端，支持 Python/JavaScript 调用   零模板代码 自动生成 API 文档和 Playground 调试界面    典型工作流 定义 LangChain 对象</description>
    </item>
    
    <item>
      <title>LCEL与LangChain流式处理指南</title>
      <link>https://zcj-git520.github.io/p/lcel%E4%B8%8Elangchain%E6%B5%81%E5%BC%8F%E5%A4%84%E7%90%86%E6%8C%87%E5%8D%97/</link>
      <pubDate>Sat, 25 Jan 2025 22:00:38 +0800</pubDate>
      
      <guid>https://zcj-git520.github.io/p/lcel%E4%B8%8Elangchain%E6%B5%81%E5%BC%8F%E5%A4%84%E7%90%86%E6%8C%87%E5%8D%97/</guid>
      <description>LCEL与LangChain流式处理指南 什么是LCEL？ LCEL（LangChain Expression Language）是一种强大的工作流编排工具，能够从基本组件构建复杂任务链条(chain)，并具有以下生产级特性：
 🚀 一流的流式支持：直接从LLM流式传输标记到输出解析器 ⚡ 异步支持：同一代码可同步/异步运行，适合原型到生产 🔀 优化的并行执行：自动并行可并行步骤 🔄 重试和回退：配置可靠性机制 🔍 中间结果访问：实时监控和调试 📐 输入输出模式：自动生成Pydantic/JSONSchema验证  Runnable接口标准 LangChain组件通过Runnable协议实现标准化调用：
同步方法  stream(): 流式返回响应块 invoke(): 调用链处理输入 batch(): 批量处理输入列表  异步方法  astream(): 异步流式响应 ainvoke(): 异步调用 abatch(): 异步批量处理 astream_log(): 流式中间步骤+最终结果 astream_events(): 流式链事件(Beta)  组件I/O类型    组件 输入类型 输出类型     提示 字典 提示值   聊天模型 字符串/消息列表/提示值 聊天消息   LLM 字符串/消息列表/提示值 字符串   输出解析器 LLM/聊天模型输出 解析器特定   检索器 字符串 文档列表   工具 字符串/字典 工具特定    流式处理(Stream) 核心方法  同步流式：stream() 异步流式：astream()  关键特性  实时返回每个处理块 要求所有步骤支持流式处理 复杂度范围：  简单：LLM令牌流 复杂：部分JSON结果流    最佳实践 建议从LLM组件开始逐步构建流式处理链。</description>
    </item>
    
    <item>
      <title>LangChain 框架解析</title>
      <link>https://zcj-git520.github.io/p/langchain-%E6%A1%86%E6%9E%B6%E8%A7%A3%E6%9E%90/</link>
      <pubDate>Sat, 18 Jan 2025 22:00:38 +0800</pubDate>
      
      <guid>https://zcj-git520.github.io/p/langchain-%E6%A1%86%E6%9E%B6%E8%A7%A3%E6%9E%90/</guid>
      <description>LangChain 框架解析 一、简介  定义：开源Python AI应用开发框架 核心价值：降低基于LLM的AI应用开发门槛 核心功能：  LLM集成（文本生成/问答/翻译/对话） 创意应用构建 标准化AI工作流    二、核心特性    特性 功能描述 应用场景     LLM &amp;amp; 提示管理 统一API抽象层 + 提示模板 标准化模型调用   链(Chain) 预封装任务工作流 问答系统/SQL生成   LCEL 表达式语言编排任务流 自定义AI流程   RAG 外部数据增强生成 知识库问答   Agents LLM决策+外部系统调用 自动化任务执行   模型记忆 对话历史记忆 多轮对话系统    LangChain 特性：
● LLM 和提示（Prompt）：LangChain 对所有 LLM 大模型进行了 API 抽象，统一了大模型访问 API，同时提供了 Prompt 提示模板管理机制。</description>
    </item>
    
  </channel>
</rss>
